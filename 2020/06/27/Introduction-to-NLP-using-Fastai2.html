<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | Through Tinted Lenses</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Immerse into the lesser known." />
<meta property="og:description" content="Immerse into the lesser known." />
<link rel="canonical" href="https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html" />
<meta property="og:url" content="https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html" />
<meta property="og:site_name" content="Through Tinted Lenses" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-27T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Immerse into the lesser known.","@type":"BlogPosting","headline":"Title","dateModified":"2020-06-27T00:00:00-05:00","datePublished":"2020-06-27T00:00:00-05:00","url":"https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/through-tinted-lenses/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://harish3110.github.io/through-tinted-lenses/feed.xml" title="Through Tinted Lenses" /><link rel="shortcut icon" type="image/x-icon" href="/through-tinted-lenses/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Title | Through Tinted Lenses</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Immerse into the lesser known." />
<meta property="og:description" content="Immerse into the lesser known." />
<link rel="canonical" href="https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html" />
<meta property="og:url" content="https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html" />
<meta property="og:site_name" content="Through Tinted Lenses" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-27T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Immerse into the lesser known.","@type":"BlogPosting","headline":"Title","dateModified":"2020-06-27T00:00:00-05:00","datePublished":"2020-06-27T00:00:00-05:00","url":"https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://harish3110.github.io/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://harish3110.github.io/through-tinted-lenses/feed.xml" title="Through Tinted Lenses" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/through-tinted-lenses/">Through Tinted Lenses</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/through-tinted-lenses/about/">About Me</a><a class="page-link" href="/through-tinted-lenses/search/">Search</a><a class="page-link" href="/through-tinted-lenses/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-27T00:00:00-05:00" itemprop="datePublished">
        Jun 27, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      19 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/harish3110/through-tinted-lenses/tree/master/_notebooks/2020-06-27-Introduction to NLP using Fastai2.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/through-tinted-lenses/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/harish3110/through-tinted-lenses/master?filepath=_notebooks%2F2020-06-27-Introduction+to+NLP+using+Fastai2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/through-tinted-lenses/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/harish3110/through-tinted-lenses/blob/master/_notebooks/2020-06-27-Introduction to NLP using Fastai2.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/through-tinted-lenses/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-06-27-Introduction to NLP using Fastai2.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h1 id="Introduction-to-NLP-using-Fastai2">Introduction to NLP using Fastai2<a class="anchor-link" href="#Introduction-to-NLP-using-Fastai2"> </a></h1><blockquote><p>Implementing the <a href="https://arxiv.org/abs/1801.06146">ULMFiT</a> approach to train a language model on any downstream NLP task.</p>
<ul>
<li>toc: true</li>
<li>branch: master</li>
<li>badges: true</li>
<li>comments: true</li>
<li>author: Harish Vadlamani</li>
<li>categories: [Natural Language Processing, Sentiment Analysis]</li>
</ul>
</blockquote>
<hr />

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="kn">from</span> <span class="nn">fastai2.text.all</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p>In continuation to my previous posts <a href="https://harish3110.github.io/through-tinted-lenses/fastai/image%20classification/2020/03/29/Building-an-image-classifier-using-Fastai-V2.html">1</a>, <a href="https://harish3110.github.io/through-tinted-lenses/fastai/image%20classification/model%20fine-tuning/2020/04/10/Improving-baseline-model.html">2</a>, which delved into the domain of computer vision by building and fine-tuning an image classification model using Fastai, I would like to venture into the fascinating domain of Natural Language Processing using Fastai.</p>
<p>For this post we'll be working on the <a href="https://www.kaggle.com/c/nlp-getting-started/overview">Real or Not? NLP with Disaster Tweets</a> competition dataset on Kaggle to build a text classifier to distinguish between normal tweets and tweets sent out during a natural disaster.</p>
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset-Download-and--basic-EDA">Dataset Download and  basic EDA<a class="anchor-link" href="#Dataset-Download-and--basic-EDA"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>

<span class="o">%</span><span class="k">cd</span> ~/Desktop/datasets
<span class="n">dataset</span> <span class="o">=</span> <span class="s1">&#39;nlp-getting-started&#39;</span>
<span class="c1"># !mkdir {dataset}</span>
<span class="o">%</span><span class="k">cd</span> {dataset}
<span class="c1"># !kaggle competitions download -c {dataset}</span>
<span class="c1"># !unzip {dataset + &#39;.zip&#39;}</span>
<span class="c1"># !rm {dataset + &#39;.zip&#39;} </span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>/home/harish3110/Desktop/datasets
/home/harish3110/Desktop/datasets/nlp-getting-started
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;train.csv&#39;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;test.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Forest fire near La Ronge Sask. Canada</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>5</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>6</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>13,000 people receive #wildfires evacuation orders in California</td>
      <td>1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>7</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0    4342
1    3271
Name: target, dtype: int64</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">test</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>keyword</th>
      <th>location</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Just happened a terrible car crash</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Heard about #earthquake is different cities, stay safe everyone.</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all</td>
    </tr>
    <tr>
      <td>3</td>
      <td>9</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Apocalypse lighting. #Spokane #wildfires</td>
    </tr>
    <tr>
      <td>4</td>
      <td>11</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The training set has 7613 records.
The test set has 3263 records.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># dataset for fine-tuning language model which only needs the text data</span>

<span class="n">df_lm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[[</span><span class="s1">&#39;text&#39;</span><span class="p">]]</span>
<span class="n">df_lm</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Forest fire near La Ronge Sask. Canada</td>
    </tr>
    <tr>
      <td>2</td>
      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected</td>
    </tr>
    <tr>
      <td>3</td>
      <td>13,000 people receive #wildfires evacuation orders in California</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="The-ULMFiT-approach">The ULMFiT approach<a class="anchor-link" href="#The-ULMFiT-approach"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Universal Language Model Fine-tuning (ULMFiT) is an inductive transfer learning approach developed by Jeremy Howard and Sebastian Ruder in this <a href="https://arxiv.org/abs/1801.06146">paper</a> to all the tasks in the domain of natural language processing which sparked the usage of transfer learning in the use of pretrained models and applying transfer learning in NLP tasks.</p>
<p><strong>The ULMFiT approach to training NLP models is heralded as the ImageNet moment in the domain of Natural Language Processing</strong></p>
<p>The model architecture used in the entire process of the ULMFiT approach is the same and is the famous <strong>AWD-LSTM</strong> architecture.</p>
<p>The ULMFiT approach can be braodly explained in the 3 major steps as shown below:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://nlp.fast.ai/images/ulmfit_approach.png" alt="" title="ULMFiT Process" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Step-1:-Training-a-general-corpus-language-model">Step 1: Training a general corpus language model<a class="anchor-link" href="#Step-1:-Training-a-general-corpus-language-model"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A language model is first trained on a corpus of Wikipedia articles known as Wikitext-103 using a <strong>self-supervised approach</strong>, i.e. using the training labels in itself to train models, in this case training a LM to learn to predict the next word in a sequence. This resulting LM learns the semantics of the english language and captures general features in the different layers.</p>
<p>This pretrained language model is trained on 28,595 Wikipedia articles and training process is very expensive and time consuming process and is luckily open-sourced in the Fastai library for us to use.</p>
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Side-Note:-Text-Pre-processing">Side Note: Text Pre-processing<a class="anchor-link" href="#Side-Note:-Text-Pre-processing"> </a></h3><blockquote><p>Transforming and normalizing texts such that it can be trained on a neural network for language modeling</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In my previous post, <a href="https://harish3110.github.io/through-tinted-lenses/fastai/image%20classification/2020/03/29/Building-an-image-classifier-using-Fastai-V2.html#1.-Create-a-DataBlock:">Building an image classifier using Fastai V2</a> we looks at the datablock API of Fastai and we see that we ensure that all images used for training the image classifier model are resized to the same size in order to be able to collate them in the GPU.</p>
<p>The same needs to be done for texts in order to train a language model. Whether it's the articles in the Wikipedia 103 dataset or tweets in disaster dataset are of different lengths and can be very long. Thus the tweets corpus i.e. the dataset needs to pre-processed correctly in order to train a neural network on text data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are many ways the pre-processing for textual data can be done and Fastai does this by applying the following 2 main transforms to texts:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><strong><em>Note:</em></strong> A transform in Fastai is basically an <strong>almost</strong> reversible function that transforms data into another form(encoding) and also has the capability of getting back the original data(decoding) if needed.</p>
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="1.-Tokenization">1. Tokenization<a class="anchor-link" href="#1.-Tokenization"> </a></h5><p>The first step is to gather all the unique 'tokens' in the corpus being used.</p>
<p>A 'token' is defined by the person creating the language model based on the granularity level i.e. the smallest part of the text they would like to consider. In the simplest scenarion, a word can be considered as the token.</p>
<p>So the idea is to get a list of all the unique words used in the general domain corpus(Wikipedia 103 dataset) and our downstream dataset(Disaster tweets dataset) to build a vocabulary for training our language model.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s take an example text from our training set to show a tokenization example</span>

<span class="n">txt</span> <span class="o">=</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">txt</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initializing the default tokenizer used in Fastai which is that of Spacy called `WordTokenizer`</span>
<span class="n">spacy</span> <span class="o">=</span> <span class="n">WordTokenizer</span><span class="p">()</span> 

<span class="c1"># Wrapping the Spacy tokenizer with a custom Fastai function to make some custom changes to the tokenizer</span>
<span class="n">tkn</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">spacy</span><span class="p">)</span> 

<span class="n">tkn</span><span class="p">(</span><span class="n">txt</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Setting up a tokenizer on the entire dataframe &#39;df_lm&#39;</span>
<span class="n">tok</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="n">df_lm</span><span class="p">)</span>
<span class="n">tok</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
<span class="n">toks</span> <span class="o">=</span> <span class="n">txts</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span>
<span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><strong>Note:</strong> The special tokens you can see above starting with 'xx' are special fastai tokens added on top of the spacy tokenizer used to indicate certain extra meanings in the text data as follows:</p>
<ul>
<li><code>xxbos</code>:: Indicates the beginning of a text (here, a review)</li>
<li><code>xxmaj</code>:: Indicates the next word begins with a capital (since we lowercased everything)</li>
<li><code>xxunk</code>:: Indicates the next word is unknown</li>
</ul>
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As mentioned above <code>Tokenizer</code> is a Fastai transform, which is basically a function with and <code>encodes</code> and <code>decodes</code> method available to tokenize a text and return it back to <strong>almost</strong> the same initial state.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok</span><span class="o">.</span><span class="n">encodes</span><span class="p">(</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...]</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tok</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;xxbos xxmaj our xxmaj deeds are the xxmaj reason of this # earthquake xxmaj may xxup allah xxmaj forgive us all&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The reason we don't get the original string back when applying <code>decode</code> is because the default tokenizer used in this case isn't <code>reversible</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h5 id="2.-Numericalization">2. Numericalization<a class="anchor-link" href="#2.-Numericalization"> </a></h5><p>The next step in the pre-processing step is to index the tokens created earlier so that they can easily accessed.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num</span> <span class="o">=</span> <span class="n">Numericalize</span><span class="p">()</span>
<span class="n">num</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">toks</span><span class="p">)</span>
<span class="n">nums</span> <span class="o">=</span> <span class="n">toks</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
<span class="n">nums</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  2,   8, 150,   8,   0,  43,  14,   8, 884,  19])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num</span><span class="o">.</span><span class="n">encodes</span><span class="p">(</span><span class="n">toks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TensorText([   2,    8,  150,    8,    0,   43,   14,    8,  884,   19,   39,   13,
         300,    8,  169,    7, 1620,    8,    0,  120,   65])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">num</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">nums</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#10) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;xxunk&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Step-2:-Fine-tuning-pretrained-LM-to-downstream-dataset">Step 2: Fine-tuning pretrained LM to downstream dataset<a class="anchor-link" href="#Step-2:-Fine-tuning-pretrained-LM-to-downstream-dataset"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Despite having a vast language model pre-trained, it's always likely that the specific downstream task we would like to build our NLP model is a part of a slightly different distribution and thus  need to fine-tune this Wikitext 103 LM.</p>
<p>This step is much faster and it converges much faster as there will be an overlap to the general domain dataset. It only needs to adapt to the idiosyncrasies of the language used and not learn the language per say.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since NLP models are more shallow in comparison to a computer vision model, the fine-tuning approaches need to be different and thus the paper provides novel fine-tuning techniques to do so:</p>
<h4 id="Discriminative-Fine-tuning">Discriminative Fine-tuning<a class="anchor-link" href="#Discriminative-Fine-tuning"> </a></h4><p>Since different layers of the model capture different types of information and thus they should be fine-tuned to different extents. This idea is same as the use of discriminative learning rates used in CV applications.</p>
<h4 id="Slanted-Learning-Rates">Slanted Learning Rates<a class="anchor-link" href="#Slanted-Learning-Rates"> </a></h4><p>The idea behing slanted learning rates is that for a pretrained language model to adpat/fine-tune itself to the downstream dataset, the fine-tuning process should ideally converge faster to asuitable region in the parameter space and thern refine its parameters there.</p>
<p>So the slanted learning rates approach first linearly increases the learning rates for a short period and then linearly decays the learning rate slowly which is a modification of of Leslie Smith's traingular learning rate approache where the increase and decrease is almost the same.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Creating-a-dataloader">Creating a dataloader<a class="anchor-link" href="#Creating-a-dataloader"> </a></h4><blockquote><p>Putting the pre-processed data in batches of text sequences for fine-tuning the language model</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Creating a dataloader for self-supervised learning task which tries to predict the next word in a sequence as represented by <code>text_</code> below.</p>
<p><strong>Fastai handles text processing steps like tokenization and numericalization internally when <code>TextBlock</code> is passed to <code>DataBlock</code>.</strong></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_lm</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="n">TextBlock</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">is_lm</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span> 
    <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span> 
    <span class="c1"># using only 10% of entire comments data for validation inorder to learn more</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><strong><em>Note:</em></strong> An important trick used in creating a dataloader here is that we use all the data available to us i.e train and test data. In case we had a dataset with unlabeled reviews we could also use that to fine-tune the pre-trained model better since this step doesn't need labels and is self-supervised.</p>
<hr />

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_lm</span> <span class="o">=</span> <span class="n">dls_lm</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">df_lm</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">72</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><strong><em>Note:</em></strong></p>
<ul>
<li>Select the batch size <code>bs</code> based on how much your GPU can handle without running out of memory</li>
<li>The sequence length <code>seq_len</code> for the data split used here is the default sequence length used for training the Wikipedia 103 language model</li>
</ul>
<hr />

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls_lm</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>text_</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>xxbos xxmaj put the xxup right person up on the block # xxmaj shelli xxrep 3 ? xxmaj the sense of xxunk is ridiculous . # xxup bb17 . xxbos xxunk xxup dw was on his way to a better career than xxmaj xxunk and that was n't derailed until 2014 . xxmaj shame . xxbos xxmaj wreckage ' conclusively xxmaj confirmed ' as xxmaj from xxup mh370 : xxmaj malaysia xxup pm : xxmaj investigators and the families of</td>
      <td>xxmaj put the xxup right person up on the block # xxmaj shelli xxrep 3 ? xxmaj the sense of xxunk is ridiculous . # xxup bb17 . xxbos xxunk xxup dw was on his way to a better career than xxmaj xxunk and that was n't derailed until 2014 . xxmaj shame . xxbos xxmaj wreckage ' conclusively xxmaj confirmed ' as xxmaj from xxup mh370 : xxmaj malaysia xxup pm : xxmaj investigators and the families of those</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xxmaj under a xxmaj minute http : / / t.co / xxunk xxbos xxmaj correction : xxmaj tent xxmaj collapse xxmaj story http : / / t.co / xxunk xxbos xxunk xxmaj xxunk not getting rid me until every last drop of life has gone … i xxunk into burning buildings because all xxmaj life matters . xxmaj xxunk xxbos xxmaj suicide bomber xxunk in xxmaj saudi xxmaj arabia mosque 17 reportedly killed http : / / t.co / xxunk</td>
      <td>under a xxmaj minute http : / / t.co / xxunk xxbos xxmaj correction : xxmaj tent xxmaj collapse xxmaj story http : / / t.co / xxunk xxbos xxunk xxmaj xxunk not getting rid me until every last drop of life has gone … i xxunk into burning buildings because all xxmaj life matters . xxmaj xxunk xxbos xxmaj suicide bomber xxunk in xxmaj saudi xxmaj arabia mosque 17 reportedly killed http : / / t.co / xxunk xxbos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>our true xxmaj hero 's ! ! xxmaj besides your music xxbos xxmaj if abortion is murder then xxunk are xxunk and xxunk is mass genocide . xxbos xxmaj fatality ! xxbos xxmaj interesting : xxup mh370 : xxmaj aircraft debris found on xxmaj la xxmaj reunion is from missing xxmaj malaysia xxmaj airlines … - xxup abc … http : / / t.co / xxunk xxmaj please xxup rt xxbos xxmaj rly tragedy in xxup mp : xxmaj some</td>
      <td>true xxmaj hero 's ! ! xxmaj besides your music xxbos xxmaj if abortion is murder then xxunk are xxunk and xxunk is mass genocide . xxbos xxmaj fatality ! xxbos xxmaj interesting : xxup mh370 : xxmaj aircraft debris found on xxmaj la xxmaj reunion is from missing xxmaj malaysia xxmaj airlines … - xxup abc … http : / / t.co / xxunk xxmaj please xxup rt xxbos xxmaj rly tragedy in xxup mp : xxmaj some live</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Saving the dataloader for fast use in the future</span>

<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">dls_lm</span><span class="p">,</span> <span class="s1">&#39;disaster_tweets_dls_lm.pkl&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># To load the Dataloaders in the future</span>

<span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;disaster_tweets_dls_lm.pkl&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;fastai2.data.core.DataLoaders at 0x7f331d425510&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Fine-tuning-the-language-model">Fine-tuning the language model<a class="anchor-link" href="#Fine-tuning-the-language-model"> </a></h4><p>Fine-tuning Wikitext 103 based LM to disaster tweets using ULMFiT fine-tuning methodologies. This fine-tuned LM can thus be used as the base to classify disaster texts in the next step.</p>
<p>The common metric used in CV models is accuracy but in sequence based models we use something called <strong>perplexity</strong> which is basically exponential of the loss as follows:</p>

<pre><code>torch.exp(cross_entropy)</code></pre>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#fine-tuning wikitext LM to disaster tweets dataset</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">language_model_learner</span><span class="p">(</span>
    <span class="n">dls_lm</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">Perplexity</span><span class="p">()])</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SequentialRNN(
  (0): AWD_LSTM(
    (encoder): Embedding(5832, 400, padding_idx=1)
    (encoder_dp): EmbeddingDropout(
      (emb): Embedding(5832, 400, padding_idx=1)
    )
    (rnns): ModuleList(
      (0): WeightDropout(
        (module): LSTM(400, 1152, batch_first=True)
      )
      (1): WeightDropout(
        (module): LSTM(1152, 1152, batch_first=True)
      )
      (2): WeightDropout(
        (module): LSTM(1152, 400, batch_first=True)
      )
    )
    (input_dp): RNNDropout()
    (hidden_dps): ModuleList(
      (0): RNNDropout()
      (1): RNNDropout()
      (2): RNNDropout()
    )
  )
  (1): LinearDecoder(
    (decoder): Linear(in_features=400, out_features=5832, bias=True)
    (output_dp): RNNDropout()
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<h5 id="Embedding-Layer">Embedding Layer<a class="anchor-link" href="#Embedding-Layer"> </a></h5><p>We can see that the above <code>AWD LSTM</code> architecture used in ULMFiT has a bunch of something called <strong>embedding</strong> layers as the input here.</p>
<p>The pre-processed text and the batching of data using dataloaders is followed by by passing this data into an embedding layer which is small neural network by itself which is used to calculate token i.e. word dependencies in the dataset. These layers are trained along with the main neural network model and learns relationships between words in the dataset.</p>
<p>An embedding layer is a computationally efficient method to represent tokens in a lesser dimension space, being less sparse and as a look-up table for all tokens in our dataset which captures relationships between the tokens.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://www.fast.ai/images/kittenavalanche.png" alt="" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the above embediing layer learned, vectors for baby animal words are closer together, and an unrelated word like 'avalanche' is further away</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you would like to know more about word embedding check out this amazing <a href="https://www.youtube.com/watch?v=25nC0n9ERq4">video</a> by Rachael Thomas, co-founder of Fastai.</p>
<hr />

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.0831763744354248)</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's train the last layer of the model using a learning rate of <code>1e-2</code> based on the above learning rate finder plot using Leslie Smith's <a href="https://arxiv.org/abs/1708.07120">1 Cycle Training</a> approach.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>perplexity</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>5.023802</td>
      <td>3.831925</td>
      <td>0.369296</td>
      <td>46.151302</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>perplexity</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>3.945078</td>
      <td>3.558439</td>
      <td>0.395056</td>
      <td>35.108360</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>1</td>
      <td>3.691576</td>
      <td>3.303960</td>
      <td>0.427633</td>
      <td>27.220207</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3.473612</td>
      <td>3.195056</td>
      <td>0.441754</td>
      <td>24.411539</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>3</td>
      <td>3.313620</td>
      <td>3.149503</td>
      <td>0.447472</td>
      <td>23.324463</td>
      <td>00:08</td>
    </tr>
    <tr>
      <td>4</td>
      <td>3.224377</td>
      <td>3.142539</td>
      <td>0.448976</td>
      <td>23.162609</td>
      <td>00:08</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once we have fine-tuned out LM to our downstream task, we save the encoder part of the model which is all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary.</p>
<p>We can then use this encoder part as our base to build a toxic comment classification model.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Saving the encoder</span>

<span class="n">learn</span><span class="o">.</span><span class="n">save_encoder</span><span class="p">(</span><span class="s1">&#39;finetuned&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Step-3:-Training-a-classifier-on-the-downstream-NLP-task">Step 3: Training a classifier on the downstream NLP task<a class="anchor-link" href="#Step-3:-Training-a-classifier-on-the-downstream-NLP-task"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have a LM fine-tuned to our downstream NLP dataset we can use the encoder portion of the fine-tuned language model which is the part that learns the features of the language used in the downstream dataset as the base to build a text classifier for tasks such as sentiment analysis, spam detection, fraud detection, document classifcation etc.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The encoder saved is then appended by a simple classifier consisting of two additional linear blocks consisting of the  standard batch normalization and dropout, with ReLU activations for the intermediate layer and a softmax activation at the last layer for the classification purpose.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fine-tuning a classifier is a very critical task in a transfer learning method. Overly aggressive fine-tuning can result in catastrophic forgetting and too cautious fine-tuning can lead to extremely slow convergence.</p>
<p>To tackle this the paper introduces <strong>gradual unfreezing</strong> besides also using slanted triangular learning rates and discriminative fine-tuning</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Gradual-Unfreezing">Gradual Unfreezing<a class="anchor-link" href="#Gradual-Unfreezing"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The idea behind gradual unfreezing is that fine-tuning a classifier on all layers can result in catastrophic forgetting and thus each layer staring form the las layer is trained one after the other by freezing all the lower layers and only training the layer in question.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Backpropagation-Throught-Time-for-Text-Classification-(BPT3C)">Backpropagation Throught Time for Text Classification (BPT3C)<a class="anchor-link" href="#Backpropagation-Throught-Time-for-Text-Classification-(BPT3C)"> </a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since the model architecture for training and fine-tuning the language is that of an LSTM, the paper implements the backpropagation through time(BPTT) approach to be able propagate gradients without them exploding or vanishing.</p>
<p>In the ULMFiT approach, a modification to the traditional BPTT is made specifically in the fine-tuning classifier phase called <strong>BPTT for Text Classification(BPT3C)</strong> top make fine-tuning a classifier for large documents feasible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Steps in BPT3C:</p>
<ul>
<li>The document is divided into fixed length batches of size 'b'. </li>
<li>At the beginning of each batch, the model is initiated with the final state of the previous batch by keeping track of the hidden states for mean and max-pooling. </li>
<li>The gradients are back-propagated to the batches whose hidden states contributed to the final prediction. </li>
<li>In practice, variable length back-propagation sequences are used. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Creating-the-classifier-dataloader">Creating the classifier dataloader<a class="anchor-link" href="#Creating-the-classifier-dataloader"> </a></h4><p>Ensure that the sequence length and vocab passed to the <code>TextBlock</code> is same as that given while fine-tuning LM</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">TextBlock</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">dls_lm</span><span class="o">.</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">dls_lm</span><span class="o">.</span><span class="n">vocab</span><span class="p">),</span> <span class="n">CategoryBlock</span><span class="p">())</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="n">blocks</span><span class="p">,</span>
                <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span>
                <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;target&#39;</span><span class="p">),</span>
                <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">show_batch</span><span class="p">(</span><span class="n">max_n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>xxbos _ \n▁ xxrep 5 ? xxup retweet \n▁ xxrep 7 ? \n▁ xxrep 5 ? xxup follow xxup all xxup who xxup rt \n▁ xxrep 7 ? \n▁ xxrep 5 ? xxup xxunk \n▁ xxrep 7 ? \n▁ xxrep 5 ? xxup gain xxup with \n▁ xxrep 7 ? \n▁ xxrep 5 ? xxup follow ? xxunk # xxup xxunk \n▁ # xxup ty</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>xxbos xxup info xxup s. xxup wnd : xxunk / 6 . xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup inst xxup apch . xxup rwy 05 . xxup curfew xxup in xxup oper xxup until 2030 xxup z. xxup taxiways xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup navbl . xxup tmp : 10 .</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>xxbos xxup info xxup u. xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup inst xxup apch . xxup rwy 05 . xxup curfew xxup in xxup oper xxup until 2030 xxup z. xxup taxiways xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup navbl . xxup tmp : 10 . xxup wnd : xxunk / 6 .</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">train_ds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">valid_ds</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(6091, 1522)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Defining-the-learner">Defining the learner<a class="anchor-link" href="#Defining-the-learner"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">text_classifier_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">,</span> <span class="n">FBeta</span><span class="p">(</span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">to_fp16</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">load_encoder</span><span class="p">(</span><span class="s1">&#39;finetuned&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;fastai2.text.learner.TextLearner at 0x7f32c7f08e10&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse-hide</span>
<span class="n">learn</span><span class="o">.</span><span class="n">model</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SequentialRNN(
  (0): SentenceEncoder(
    (module): AWD_LSTM(
      (encoder): Embedding(5832, 400, padding_idx=1)
      (encoder_dp): EmbeddingDropout(
        (emb): Embedding(5832, 400, padding_idx=1)
      )
      (rnns): ModuleList(
        (0): WeightDropout(
          (module): LSTM(400, 1152, batch_first=True)
        )
        (1): WeightDropout(
          (module): LSTM(1152, 1152, batch_first=True)
        )
        (2): WeightDropout(
          (module): LSTM(1152, 400, batch_first=True)
        )
      )
      (input_dp): RNNDropout()
      (hidden_dps): ModuleList(
        (0): RNNDropout()
        (1): RNNDropout()
        (2): RNNDropout()
      )
    )
  )
  (1): PoolingLinearClassifier(
    (layers): Sequential(
      (0): LinBnDrop(
        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Dropout(p=0.2, inplace=False)
        (2): Linear(in_features=1200, out_features=50, bias=False)
        (3): ReLU(inplace=True)
      )
      (1): LinBnDrop(
        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): Dropout(p=0.1, inplace=False)
        (2): Linear(in_features=50, out_features=2, bias=False)
      )
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Training-the-classifier">Training the classifier<a class="anchor-link" href="#Training-the-classifier"> </a></h4>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>fbeta_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.650071</td>
      <td>0.471966</td>
      <td>0.777267</td>
      <td>0.706494</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Applying gradual unfreezing of one layer after another</span>

<span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>fbeta_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.564487</td>
      <td>0.442714</td>
      <td>0.796321</td>
      <td>0.752000</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">freeze_to</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">)</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">5e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">1e-2</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>fbeta_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.500388</td>
      <td>0.430654</td>
      <td>0.801577</td>
      <td>0.758786</td>
      <td>00:02</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
<span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mf">1e-3</span><span class="o">/</span><span class="p">(</span><span class="mf">2.6</span><span class="o">**</span><span class="mi">4</span><span class="p">),</span><span class="mf">3e-3</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>fbeta_score</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.449139</td>
      <td>0.438118</td>
      <td>0.796978</td>
      <td>0.756117</td>
      <td>00:03</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.426530</td>
      <td>0.435094</td>
      <td>0.800920</td>
      <td>0.772352</td>
      <td>00:03</td>
    </tr>
  </tbody>
</table>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Kaggle-Submission">Kaggle Submission<a class="anchor-link" href="#Kaggle-Submission"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sub</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;sample_submission.csv&#39;</span><span class="p">)</span>
<span class="n">sub</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>9</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>11</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dl</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">dls</span><span class="o">.</span><span class="n">test_dl</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">learn</span><span class="o">.</span><span class="n">get_preds</span><span class="p">(</span><span class="n">dl</span><span class="o">=</span><span class="n">dl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s view the output of a single row of data</span>

<span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([0.07068779, 0.9293122 ], dtype=float32)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Since it&#39;s a multi-class problem and it uses softmax on the binary classes, </span>
<span class="c1"># Need to calculate argmax of the output to get the best class as follows </span>

<span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(1)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sub</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sub</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>9</td>
      <td>1</td>
    </tr>
    <tr>
      <td>4</td>
      <td>11</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sub</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;~/Desktop/my_fastai_notebooks/Text/submission.csv&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above submission acheived a score of 0.80447 on the competition leaderboard.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><ul>
<li>Fastai v2 <a href="www.dev.fasta.ai">Documentation</a></li>
<li>Fastbook Chapter 10: <a href="https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb">NLP Deep Dive</a></li>
</ul>
<hr />
<p>Happy learning, stay at home and stay safe! :)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/through-tinted-lenses/2020/06/27/Introduction-to-NLP-using-Fastai2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/through-tinted-lenses/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/through-tinted-lenses/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/through-tinted-lenses/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Immerse into the lesser known.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/harish3110" title="harish3110"><svg class="svg-icon grey"><use xlink:href="/through-tinted-lenses/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/harish3110" title="harish3110"><svg class="svg-icon grey"><use xlink:href="/through-tinted-lenses/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
