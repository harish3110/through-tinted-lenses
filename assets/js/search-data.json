{
  
    
        "post0": {
            "title": "Introduction to NLP using Fastai",
            "content": ". #collapse-hide # Installing and importing the necessary libraries !pip install fastai2 --quiet !pip install kaggle --quiet from fastai2.text.all import * import warnings warnings.filterwarnings(&#39;ignore&#39;) . . In continuation to my previous posts 1, 2, which delved into the domain of computer vision by building and fine-tuning an image classification model using Fastai, I would like to venture into the fascinating domain of Natural Language Processing using Fastai. . For this post we&#39;ll be working on the Real or Not? NLP with Disaster Tweets competition dataset on Kaggle to build a text classifier to distinguish between normal tweets and tweets sent out during a natural disaster using the ULMFiT approach. . . Dataset Download and basic EDA . Using Kaggle API to download the competition dataset and view the data . # Using the kaggle api to search the name of the competition dataset to download !kaggle competitions list -s &#39;nlp&#39; . ref deadline category reward teamCount userHasEntered - - -- nlp-getting-started 2030-01-01 00:00:00 Getting Started Kudos 1577 True trec-covid-information-retrieval 2020-06-03 11:00:00 Research Kudos 19 False google-quest-challenge 2020-02-10 23:59:00 Featured $25,000 1571 False jigsaw-unintended-bias-in-toxicity-classification 2019-07-18 19:35:00 Featured $65,000 3165 False gendered-pronoun-resolution 2019-04-22 23:59:00 Research $25,000 838 False word2vec-nlp-tutorial 2015-06-30 23:59:00 Getting Started Knowledge 577 False data-science-for-good-city-of-los-angeles 2019-06-21 23:59:00 Analytics $15,000 0 False . The competition dataset we would like to download is the 1st one titled nlp-getting-started . %cd ~/Desktop/datasets dataset = &#39;nlp-getting-started&#39; # Creating a folder for the dataset !mkdir {dataset} %cd {dataset} # Using the Kaggle API to download dataset !kaggle competitions download -c {dataset} . /home/harish3110/Desktop/datasets /home/harish3110/Desktop/datasets/nlp-getting-started Downloading nlp-getting-started.zip to /home/harish3110/Desktop/datasets/nlp-getting-started 100%|████████████████████████████████████████| 593k/593k [00:00&lt;00:00, 2.11MB/s] 100%|████████████████████████████████████████| 593k/593k [00:00&lt;00:00, 2.10MB/s] . # Unzip the dataset and delete the respective zip file !unzip {dataset + &#39;.zip&#39;} !rm {dataset + &#39;.zip&#39;} . Archive: nlp-getting-started.zip inflating: sample_submission.csv inflating: test.csv inflating: train.csv . # View the files !ls . sample_submission.csv test.csv train.csv . train = pd.read_csv(&#39;train.csv&#39;) test = pd.read_csv(&#39;test.csv&#39;) . train.head() . id keyword location text target . 0 | 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all | 1 | . 1 | 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 | 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are being notified by officers. No other evacuation or shelter in place orders are expected | 1 | . 3 | 6 | NaN | NaN | 13,000 people receive #wildfires evacuation orders in California | 1 | . 4 | 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school | 1 | . train[&#39;target&#39;].value_counts() . 0 4342 1 3271 Name: target, dtype: int64 . test.head() . id keyword location text . 0 | 0 | NaN | NaN | Just happened a terrible car crash | . 1 | 2 | NaN | NaN | Heard about #earthquake is different cities, stay safe everyone. | . 2 | 3 | NaN | NaN | there is a forest fire at spot pond, geese are fleeing across the street, I cannot save them all | . 3 | 9 | NaN | NaN | Apocalypse lighting. #Spokane #wildfires | . 4 | 11 | NaN | NaN | Typhoon Soudelor kills 28 in China and Taiwan | . The training set has 7613 records. The test set has 3263 records. . # dataset for fine-tuning language model which only needs the text data df_lm = pd.concat([train, test], axis=0)[[&#39;text&#39;]] df_lm.head() . text . 0 | Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all | . 1 | Forest fire near La Ronge Sask. Canada | . 2 | All residents asked to &#39;shelter in place&#39; are being notified by officers. No other evacuation or shelter in place orders are expected | . 3 | 13,000 people receive #wildfires evacuation orders in California | . 4 | Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school | . . The ULMFiT approach . The Universal Language Model Fine-tuning (ULMFiT) is an inductive transfer learning approach developed by Jeremy Howard and Sebastian Ruder in this paper to all the tasks in the domain of natural language processing which sparked the usage of transfer learning in the use of pretrained models and applying transfer learning in NLP tasks. . The ULMFiT approach to training NLP models is heralded as the ImageNet moment in the domain of Natural Language Processing . The model architecture used in the entire process of the ULMFiT approach is the same and is the famous AWD-LSTM architecture. . The ULMFiT approach can be braodly explained in the 3 major steps as shown below: . . . Step 1: Training a general corpus language model . A language model is first trained on a corpus of Wikipedia articles known as Wikitext-103 using a self-supervised approach, i.e. using the training labels in itself to train models, in this case training a LM to learn to predict the next word in a sequence. This resulting LM learns the semantics of the english language and captures general features in the different layers. . This pretrained language model is trained on 28,595 Wikipedia articles and training process is very expensive and time consuming process and is luckily open-sourced in the Fastai library for us to use. . . Side Note: Text Pre-processing . Transforming and normalizing texts such that it can be trained on a neural network for language modeling . In my previous post, Building an image classifier using Fastai V2 we looks at the datablock API of Fastai and we see that we ensure that all images used for training the image classifier model are resized to the same size in order to be able to collate them in the GPU. . The same needs to be done for texts in order to train a language model. Whether it&#39;s the articles in the Wikipedia 103 dataset or tweets in disaster dataset are of different lengths and can be very long. Thus the tweets corpus i.e. the dataset needs to pre-processed correctly in order to train a neural network on text data. . There are many ways the pre-processing for textual data can be done and Fastai does this by applying the following 2 main transforms to texts: . . Note: A transform in Fastai is basically an almost reversible function that transforms data into another form(encoding) and also has the capability of getting back the original data(decoding) if needed. . . 1. Tokenization . The first step is to gather all the unique &#39;tokens&#39; in the corpus being used. . A &#39;token&#39; is defined by the person creating the language model based on the granularity level i.e. the smallest part of the text they would like to consider. In the simplest scenarion, a word can be considered as the token. . So the idea is to get a list of all the unique words used in the general domain corpus(Wikipedia 103 dataset) and our downstream dataset(Disaster tweets dataset) to build a vocabulary for training our language model. . # Let&#39;s take an example text from our training set to show a tokenization example txt = train[&#39;text&#39;].iloc[0] txt . &#39;Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all&#39; . # Initializing the default tokenizer used in Fastai which is that of Spacy called `WordTokenizer` spacy = WordTokenizer() # Wrapping the Spacy tokenizer with a custom Fastai function to make some custom changes to the tokenizer tkn = Tokenizer(spacy) tkn(txt) . (#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...] . # Setting up a tokenizer on the entire dataframe &#39;df_lm&#39; tok = Tokenizer.from_df(df_lm) tok.setup(train) toks = txts.map(tok) toks[0] . (#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...] . . Note: The special tokens you can see above starting with &#39;xx&#39; are special fastai tokens added on top of the spacy tokenizer used to indicate certain extra meanings in the text data as follows: . xxbos:: Indicates the beginning of a text (here, a review) | xxmaj:: Indicates the next word begins with a capital (since we lowercased everything) | xxunk:: Indicates the next word is unknown | . . As mentioned above Tokenizer is a Fastai transform, which is basically a function with and encodes and decodes method available to tokenize a text and return it back to almost the same initial state. . tok.encodes(toks[0]) . (#21) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;deeds&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;...] . tok.decode(toks[0]) . &#39;xxbos xxmaj our xxmaj deeds are the xxmaj reason of this # earthquake xxmaj may xxup allah xxmaj forgive us all&#39; . The reason we don&#39;t get the original string back when applying decode is because the default tokenizer used in this case isn&#39;t reversible. . 2. Numericalization . The next step in the pre-processing step is to index the tokens created earlier so that they can easily accessed. . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . tensor([ 2, 8, 150, 8, 0, 43, 14, 8, 884, 19]) . num.encodes(toks[0]) . TensorText([ 2, 8, 150, 8, 0, 43, 14, 8, 884, 19, 39, 13, 300, 8, 169, 7, 1620, 8, 0, 120, 65]) . num.decode(nums[0][:10]) . (#10) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;our&#39;,&#39;xxmaj&#39;,&#39;xxunk&#39;,&#39;are&#39;,&#39;the&#39;,&#39;xxmaj&#39;,&#39;reason&#39;,&#39;of&#39;] . . Step 2: Fine-tuning pretrained LM to downstream dataset . Despite having a vast language model pre-trained, it&#39;s always likely that the specific downstream task we would like to build our NLP model is a part of a slightly different distribution and thus need to fine-tune this Wikitext 103 LM. . This step is much faster and it converges much faster as there will be an overlap to the general domain dataset. It only needs to adapt to the idiosyncrasies of the language used and not learn the language per say. . Since NLP models are more shallow in comparison to a computer vision model, the fine-tuning approaches need to be different and thus the paper provides novel fine-tuning techniques to do so: . Discriminative Fine-tuning . Since different layers of the model capture different types of information and thus they should be fine-tuned to different extents. This idea is same as the use of discriminative learning rates used in CV applications. . Slanted Learning Rates . The idea behing slanted learning rates is that for a pretrained language model to adpat/fine-tune itself to the downstream dataset, the fine-tuning process should ideally converge faster to asuitable region in the parameter space and thern refine its parameters there. . So the slanted learning rates approach first linearly increases the learning rates for a short period and then linearly decays the learning rate slowly which is a modification of of Leslie Smith&#39;s traingular learning rate approache where the increase and decrease is almost the same. . Creating a dataloader . Putting the pre-processed data in batches of text sequences for fine-tuning the language model . Creating a dataloader for self-supervised learning task which tries to predict the next word in a sequence as represented by text_ below. . Fastai handles text processing steps like tokenization and numericalization internally when TextBlock is passed to DataBlock. . dls_lm = DataBlock( blocks=TextBlock.from_df(&#39;text&#39;, is_lm=True), get_x=ColReader(&#39;text&#39;), splitter=RandomSplitter(0.1) # using only 10% of entire comments data for validation inorder to learn more ) . . Note: An important trick used in creating a dataloader here is that we use all the data available to us i.e train and test data. In case we had a dataset with unlabeled reviews we could also use that to fine-tune the pre-trained model better since this step doesn&#39;t need labels and is self-supervised. . . dls_lm = dls_lm.dataloaders(df_lm, bs=64, seq_len=72) . . Note: . Select the batch size bs based on how much your GPU can handle without running out of memory | The sequence length seq_len for the data split used here is the default sequence length used for training the Wikipedia 103 language model | . . dls_lm.show_batch(max_n=3) . text text_ . 0 xxbos xxmaj put the xxup right person up on the block # xxmaj shelli xxrep 3 ? xxmaj the sense of xxunk is ridiculous . # xxup bb17 . xxbos xxunk xxup dw was on his way to a better career than xxmaj xxunk and that was n&#39;t derailed until 2014 . xxmaj shame . xxbos xxmaj wreckage &#39; conclusively xxmaj confirmed &#39; as xxmaj from xxup mh370 : xxmaj malaysia xxup pm : xxmaj investigators and the families of | xxmaj put the xxup right person up on the block # xxmaj shelli xxrep 3 ? xxmaj the sense of xxunk is ridiculous . # xxup bb17 . xxbos xxunk xxup dw was on his way to a better career than xxmaj xxunk and that was n&#39;t derailed until 2014 . xxmaj shame . xxbos xxmaj wreckage &#39; conclusively xxmaj confirmed &#39; as xxmaj from xxup mh370 : xxmaj malaysia xxup pm : xxmaj investigators and the families of those | . 1 xxmaj under a xxmaj minute http : / / t.co / xxunk xxbos xxmaj correction : xxmaj tent xxmaj collapse xxmaj story http : / / t.co / xxunk xxbos xxunk xxmaj xxunk not getting rid me until every last drop of life has gone … i xxunk into burning buildings because all xxmaj life matters . xxmaj xxunk xxbos xxmaj suicide bomber xxunk in xxmaj saudi xxmaj arabia mosque 17 reportedly killed http : / / t.co / xxunk | under a xxmaj minute http : / / t.co / xxunk xxbos xxmaj correction : xxmaj tent xxmaj collapse xxmaj story http : / / t.co / xxunk xxbos xxunk xxmaj xxunk not getting rid me until every last drop of life has gone … i xxunk into burning buildings because all xxmaj life matters . xxmaj xxunk xxbos xxmaj suicide bomber xxunk in xxmaj saudi xxmaj arabia mosque 17 reportedly killed http : / / t.co / xxunk xxbos | . 2 our true xxmaj hero &#39;s ! ! xxmaj besides your music xxbos xxmaj if abortion is murder then xxunk are xxunk and xxunk is mass genocide . xxbos xxmaj fatality ! xxbos xxmaj interesting : xxup mh370 : xxmaj aircraft debris found on xxmaj la xxmaj reunion is from missing xxmaj malaysia xxmaj airlines … - xxup abc … http : / / t.co / xxunk xxmaj please xxup rt xxbos xxmaj rly tragedy in xxup mp : xxmaj some | true xxmaj hero &#39;s ! ! xxmaj besides your music xxbos xxmaj if abortion is murder then xxunk are xxunk and xxunk is mass genocide . xxbos xxmaj fatality ! xxbos xxmaj interesting : xxup mh370 : xxmaj aircraft debris found on xxmaj la xxmaj reunion is from missing xxmaj malaysia xxmaj airlines … - xxup abc … http : / / t.co / xxunk xxmaj please xxup rt xxbos xxmaj rly tragedy in xxup mp : xxmaj some live | . # Saving the dataloader for fast use in the future torch.save(dls_lm, &#39;disaster_tweets_dls_lm.pkl&#39;) . # To load the Dataloaders in the future dls_lm = torch.load(&#39;disaster_tweets_dls_lm.pkl&#39;) . &lt;fastai2.data.core.DataLoaders at 0x7f331d425510&gt; . Fine-tuning the language model . Fine-tuning Wikitext 103 based LM to disaster tweets using ULMFiT fine-tuning methodologies. This fine-tuned LM can thus be used as the base to classify disaster texts in the next step. . The common metric used in CV models is accuracy but in sequence based models we use something called perplexity which is basically exponential of the loss as follows: . torch.exp(cross_entropy) . #fine-tuning wikitext LM to disaster tweets dataset learn = language_model_learner( dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()]).to_fp16() . #collapse-hide learn.model . . SequentialRNN( (0): AWD_LSTM( (encoder): Embedding(5832, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(5832, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) (1): LinearDecoder( (decoder): Linear(in_features=400, out_features=5832, bias=True) (output_dp): RNNDropout() ) ) . . Embedding Layer . We can see that the above AWD LSTM architecture used in ULMFiT has a bunch of something called embedding layers as the input here. . The pre-processed text and the batching of data using dataloaders is followed by by passing this data into an embedding layer which is small neural network by itself which is used to calculate token i.e. word dependencies in the dataset. These layers are trained along with the main neural network model and learns relationships between words in the dataset. . An embedding layer is a computationally efficient method to represent tokens in a lesser dimension space, being less sparse and as a look-up table for all tokens in our dataset which captures relationships between the tokens. . . If you would like to know more about word embedding check out this amazing video by Rachael Thomas, co-founder of Fastai. . . learn.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.0831763744354248) . Let&#39;s train the last layer of the model using a learning rate of 1e-2 based on the above learning rate finder plot using Leslie Smith&#39;s 1 Cycle Training approach. . learn.fine_tune(5, 1e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 5.023802 | 3.831925 | 0.369296 | 46.151302 | 00:08 | . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.945078 | 3.558439 | 0.395056 | 35.108360 | 00:08 | . 1 | 3.691576 | 3.303960 | 0.427633 | 27.220207 | 00:08 | . 2 | 3.473612 | 3.195056 | 0.441754 | 24.411539 | 00:08 | . 3 | 3.313620 | 3.149503 | 0.447472 | 23.324463 | 00:08 | . 4 | 3.224377 | 3.142539 | 0.448976 | 23.162609 | 00:08 | . Once we have fine-tuned out LM to our downstream task, we save the encoder part of the model which is all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. . We can then use this encoder part as our base to build a toxic comment classification model. . # Saving the encoder learn.save_encoder(&#39;finetuned&#39;) . . Step 3: Training a classifier on the downstream NLP task . Now that we have a LM fine-tuned to our downstream NLP dataset we can use the encoder portion of the fine-tuned language model which is the part that learns the features of the language used in the downstream dataset as the base to build a text classifier for tasks such as sentiment analysis, spam detection, fraud detection, document classifcation etc. . The encoder saved is then appended by a simple classifier consisting of two additional linear blocks consisting of the standard batch normalization and dropout, with ReLU activations for the intermediate layer and a softmax activation at the last layer for the classification purpose. . Fine-tuning a classifier is a very critical task in a transfer learning method. Overly aggressive fine-tuning can result in catastrophic forgetting and too cautious fine-tuning can lead to extremely slow convergence. . To tackle this the paper introduces gradual unfreezing besides also using slanted triangular learning rates and discriminative fine-tuning . Gradual Unfreezing . The idea behind gradual unfreezing is that fine-tuning a classifier on all layers can result in catastrophic forgetting and thus each layer staring form the las layer is trained one after the other by freezing all the lower layers and only training the layer in question. . Backpropagation Throught Time for Text Classification (BPT3C) . Since the model architecture for training and fine-tuning the language is that of an LSTM, the paper implements the backpropagation through time(BPTT) approach to be able propagate gradients without them exploding or vanishing. . In the ULMFiT approach, a modification to the traditional BPTT is made specifically in the fine-tuning classifier phase called BPTT for Text Classification(BPT3C) top make fine-tuning a classifier for large documents feasible. . Steps in BPT3C: . The document is divided into fixed length batches of size &#39;b&#39;. | At the beginning of each batch, the model is initiated with the final state of the previous batch by keeping track of the hidden states for mean and max-pooling. | The gradients are back-propagated to the batches whose hidden states contributed to the final prediction. | In practice, variable length back-propagation sequences are used. | . Creating the classifier dataloader . Ensure that the sequence length and vocab passed to the TextBlock is same as that given while fine-tuning LM . blocks = (TextBlock.from_df(&#39;text&#39;, seq_len=dls_lm.seq_len, vocab=dls_lm.vocab), CategoryBlock()) dls = DataBlock(blocks=blocks, get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;target&#39;), splitter=RandomSplitter(0.2)) . dls = dls.dataloaders(train, bs=64) . dls.show_batch(max_n=3) . text category . 0 xxbos _ n▁ xxrep 5 ? xxup retweet n▁ xxrep 7 ? n▁ xxrep 5 ? xxup follow xxup all xxup who xxup rt n▁ xxrep 7 ? n▁ xxrep 5 ? xxup xxunk n▁ xxrep 7 ? n▁ xxrep 5 ? xxup gain xxup with n▁ xxrep 7 ? n▁ xxrep 5 ? xxup follow ? xxunk # xxup xxunk n▁ # xxup ty | 0 | . 1 xxbos xxup info xxup s. xxup wnd : xxunk / 6 . xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup inst xxup apch . xxup rwy 05 . xxup curfew xxup in xxup oper xxup until 2030 xxup z. xxup taxiways xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup navbl . xxup tmp : 10 . | 0 | . 2 xxbos xxup info xxup u. xxup xxunk : xxup xxunk xxup xxunk . xxup exp xxup inst xxup apch . xxup rwy 05 . xxup curfew xxup in xxup oper xxup until 2030 xxup z. xxup taxiways xxup foxtrot 5 &amp; &amp; xxup foxtrot 6 xxup navbl . xxup tmp : 10 . xxup wnd : xxunk / 6 . | 0 | . len(dls.train_ds), len(dls.valid_ds) . (6091, 1522) . Defining the learner . learn = text_classifier_learner(dls, AWD_LSTM, metrics=[accuracy, FBeta(beta=1)]).to_fp16() learn.load_encoder(&#39;finetuned&#39;) . &lt;fastai2.text.learner.TextLearner at 0x7f32c7f08e10&gt; . #collapse-hide learn.model . . SequentialRNN( (0): SentenceEncoder( (module): AWD_LSTM( (encoder): Embedding(5832, 400, padding_idx=1) (encoder_dp): EmbeddingDropout( (emb): Embedding(5832, 400, padding_idx=1) ) (rnns): ModuleList( (0): WeightDropout( (module): LSTM(400, 1152, batch_first=True) ) (1): WeightDropout( (module): LSTM(1152, 1152, batch_first=True) ) (2): WeightDropout( (module): LSTM(1152, 400, batch_first=True) ) ) (input_dp): RNNDropout() (hidden_dps): ModuleList( (0): RNNDropout() (1): RNNDropout() (2): RNNDropout() ) ) ) (1): PoolingLinearClassifier( (layers): Sequential( (0): LinBnDrop( (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.2, inplace=False) (2): Linear(in_features=1200, out_features=50, bias=False) (3): ReLU(inplace=True) ) (1): LinBnDrop( (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): Dropout(p=0.1, inplace=False) (2): Linear(in_features=50, out_features=2, bias=False) ) ) ) ) . Training the classifier . learn.fit_one_cycle(1, 1e-2) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.650071 | 0.471966 | 0.777267 | 0.706494 | 00:02 | . # Applying gradual unfreezing of one layer after another learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-3/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.564487 | 0.442714 | 0.796321 | 0.752000 | 00:02 | . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.500388 | 0.430654 | 0.801577 | 0.758786 | 00:02 | . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),3e-3)) . epoch train_loss valid_loss accuracy fbeta_score time . 0 | 0.449139 | 0.438118 | 0.796978 | 0.756117 | 00:03 | . 1 | 0.426530 | 0.435094 | 0.800920 | 0.772352 | 00:03 | . . Creating a Kaggle submission file . sub = pd.read_csv(&#39;sample_submission.csv&#39;) sub.head() . id target . 0 | 0 | 0 | . 1 | 2 | 0 | . 2 | 3 | 0 | . 3 | 9 | 0 | . 4 | 11 | 0 | . dl = learn.dls.test_dl(test[&#39;text&#39;]) . preds = learn.get_preds(dl=dl) . # Let&#39;s view the output of a single row of data preds[0][0].cpu().numpy() . array([0.07068779, 0.9293122 ], dtype=float32) . # Since it&#39;s a multi-class problem and it uses softmax on the binary classes, # Need to calculate argmax of the output to get the best class as follows preds[0][0].cpu().argmax(dim=-1) . tensor(1) . sub[&#39;target&#39;] = preds[0].argmax(dim=-1) . sub.head() . id target . 0 | 0 | 1 | . 1 | 2 | 1 | . 2 | 3 | 1 | . 3 | 9 | 1 | . 4 | 11 | 1 | . sub.to_csv(&#39;~/Desktop/my_fastai_notebooks/Text/submission.csv&#39;, index=False) . The above submission acheived a score of 0.80447 on the competition leaderboard. . . Conclusion . In this post we have seen how to build a fine-tuned language model for any textual data corpus which captures the semantics of the dataset. The encoder part of this fine-tuned language model was then used to build a pretty-decent text classifier that can identify tweets describing a natural disaster. . In the upcoming posts, I would try to delve deeper into building advanced NLP models like the Transformer architecture and researching other famous research papers in the domain of NLP. . . References . Fastai v2 Documentation | Fastbook Chapter 10: NLP Deep Dive | . . Happy learning, stay at home and stay safe! :) . .",
            "url": "https://harish3110.github.io/through-tinted-lenses/natural%20language%20processing/sentiment%20analysis/2020/06/27/Introduction-to-NLP-using-Fastai.html",
            "relUrl": "/natural%20language%20processing/sentiment%20analysis/2020/06/27/Introduction-to-NLP-using-Fastai.html",
            "date": " • Jun 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing Prometheus, my very own deep learning arsenal.",
            "content": "“In Greek mythology, Prometheus, meaning “forethought” is a Titan, a culture hero, and a trickster figure who is credited with the creation of humanity from clay, and who defies the gods by stealing fire from Zeus and giving it to humanity. Prometheus is known for his intelligence and as a champion of humankind and also seen as the author of the human arts and sciences generally.” . . . Why own a Deep Learning workstation? . Today, there is a myriad of cloud GPU options one can use to dabble in the field of deep learning. Over the past year or so, I have used almost all the possible options available which have helped me grasp the concepts of the field as it has made it easy for anyone to prototype the knowledge acquired in the field at an affordable cost. Most of these services also offer free tier options and provide almost immediate access to a GPU instance. . My overall favorite has been Google’s GCP which also provides a 300$ credit to all their users as well as the newly launched Colab Pro which in my opinion is the best value for money option for most DL enthusiasts as it gives you access to multiple GPU instances for a nominal monthly fee. GCP’s main advantage is full command-line access to your Linux based server whereas Colab’s interface is quite intuitive as well. Until recently, I had opted for the Colab Pro for most of my prototyping work and finally renting a GCP instance to train large models on an hourly basis. . This being said, there are still downsides to this setup which becomes way too evident as you start spending long hours with a setup as mentioned above which I’m sure anyone in the field long enough can easily vouch for. Even though gaining access to a GPU instance has become way too easy, here the major frustration points which have bugged me long enough to shell out and invest in building my own DL rig: . The setup process of getting your instance started with the correct requirements can be quite tedious and takes some time to get a hang of things in each cloud platform available. | Preemptible instances, which are the free-tier option provided by cloud services can get annoying as you can be kicked off the instance abruptly in the middle of your training process. | On the other hand, the constant worry to ensure shutting down your device to avoid racking up the bill is real. | . In short, having your workstation at hand gives you the much-needed freedom to prototype models at the flick of your fingers and gets rid of all the friction involved in using the cloud options available. This is an extension to James Clear’s advice ‘make it easy’ which is basically to eliminate the friction involved in performing long-term rewarding habits. . “Human behavior follows the Law of Least Effort. Reduce the friction associated with good habits. When friction is low, habits are easy.” ― James Clear . . Should you build your workstation? . Due to my lack of experience with hardware components and have not built a PC till now, I was nervous to build my rig without the help of people with experience. There are many dealers and companies that provide services of building DL workstation either as a fixed option or also provide options for customizing your build. . Starting, I was certain that this was the right choice considering that the parts are quite expensive and it’s natural to feel nervous that you might fry a component and thus waste loads of money. So, I started out researching all the components needed to build a decent DL workstation. Here again, I would like to point back to another Tim Dettmer’s blog, A Full Hardware Guide to Deep Learning, which shaped my thought process in building Prometheus. . This was followed by endless hours of watching various PC builds on YouTube and learning more and more about the fascinating hobby of PC building. These hours spent watching people such jaw-dropping builds motivated be so much in wanting to build my PC and the more I saw, the less daunting the whole task seemed. I highly recommend watching builds based on the components you decide specifically by Linus Tech Tips, Jays Two Cents, and BitWit. . If you would like an all-in-one resource to build a PC based on the components needed to build a DL workstation like mine, this is the only one you’ll probably need: . How to Build a PC! Step-by-step by BitWit . In hindsight, I can confidently say that building my PC has been a wonderful learning experience that I am glad I partook in. Despite one’s fears, it is worth pointing out that the process despite seeming pretty intimidating is quite easy if one follows the right guides. The components involved in building the PC are expensive and touted as quite delicate but in all fairness, all the parts available today are quite rugged and one can dispel unnecessary fears of damaging them. Having built the entire system on my own, I have full control over any future expansion plans I have on this build and know exactly what the process is. I’m another in the long line of individuals who would like to take PC building as a valuable hobby. . This being said, If you’re the type of person who doesn’t envy this process and don’t want to deal with the hassle of building your machine, I would suggest checking out Ant-PC in India who provide customizable builds specific to deep learning and also provide on-site warranty and they thoroughly stress test their parts before shipping for an affordable premium cost in comparison. . . Components . . CPU and Motherboard . The heart of every workstation depends on these two components and the decision between choosing AMD or Intel would be the main decision that would pave the way for the rest of the components you choose. | In the last couple of years, AMD has made great strides in overthrowing up Intel off its perch by providing high-performance chips at competitive prices. | I went with the AMD route and chose the Ryzen 7 3700x card which is probably the best option in terms of performance and cost. It’s an 8 core processor with 12 threads with support for up to 128Gb in memory. | In terms of the motherboard there are way too many options available in all budget ranges but if you’re going for a premium build then going for an X570 board ensures future-proofing as they support PCIe Gen 4 and come with multiple M.2 slots for super-fast SSD storage options. | Look for X570 boards that have decent VRM(Voltage Regulated Modules) which is solely responsible for supplying stable voltage to your motherboard. | The best value proposition X570 board with great VRM’s is the ASUS TUF Gaming X570 option, which was my first choice for my build, but due to the lack of parts available owing to the current global pandemic, I had to increase my budget and go for a higher-end, Aorus X570 Ultra motherboard. | The Aorus Ultra board has pretty amazing VRM’s, 3 full-capacity PCIe 4 M.2 slots, supports NVIDIA 2 way SLI, Wi-Fi 6, Bluetooth 5.0, and premium Realtek audio. It’s one of the best mid-range X570 boards in the market today and despite the added cost over an ASUS TUF Gaming board, it provides enough expansion and future-proofing for years to come. | . Note: Despite the notion among many, an AMD setup supports all the popular deep learning libraries as efficiently as Intel boards. . Storage . It’s vital to go for an SSD to put your OS on, and ponying up for a good quality M.2 NVME SSD over the older 2.5’ SATA SSD makes a big difference in transfer speed. | For storage purposes, going for an HDD would make sense. | I got a 500GB Samsung 970 Evo Plus M.2 NVMe SSD to put Linux on for my main use and a 1 TB 7200 RPM WD Blue HDD for storage and putting on Windows. | I will look out for deals to add another SSD down the lane for loading Windows on for a faster and smoother experience. | . GPU . Arguably the most important component nowadays in a PC build, it is especially important for my use case since almost all the heavy lifting of training an ML model nowadays is done by it. | It comprises almost 50% of the overall cost of the build and thus deciding this component is one of the most important aspects to building any deep learning rig. | My insight into choosing the GPU is heavily shaped by Tim Dettmer’s blog post where he compares the price to performance of the latest NVIDIA cards. | In the blog post which is a little dated today, he recommends the RTX 2070 as the sweet spot in terms of cost-efficiency and performance. But since the release of the 20 series RTX Super cards which provide much better performance for mot much a difference. | I opted for the RTX 2070 Super and specifically chose Gigabyte’s RTX 2070 Super Windforce OC, which is highly regarded as one of the best cost-efficient GPU in the market today. | The recent NVIDIA cards since the 1080Ti offer the possibility of training DL models at half-precision points i.e. allow the possibility of training in FP16 over full precision training at FP32 which has been vital for training large models much quicker and lowering the GPU RAM usage since in theory this type of training doubles the GPU RAM at your disposal. | I would have loved to opt for the RTX 2080Ti but in all honesty, couldn’t fathom and justify the price difference between the two. I have developed my build in such a way that it can support multiple GPU systems, so maybe down the road ;). | . Power Supply . Choosing a PSU is pretty straightforward, just lookout for a decent brand and ensure that you have bought a PSU that manages to adequately supply all the components you have and would like to have in the future. | To keep an expansion possibility of adding another GPU, I went for an Antec HCP 1000W PSU, which is a fully modular power supply with an 80+ platinum rating with decently premium black sleeved cables. | For setups with a single GPU, a 750W PSU is more than sufficient but it’s worth ensuring that the PSU is either 80+ gold rated for reliable power supply to your PC. | . RAM . When choosing a RAM, the main criteria to ensure is to have enough RAM to perform your prototyping smoothly. The clock speeds and latency aren’t as important. | For Intel processors, one could easily go for lower RAM speeds but AMD suggests users go for higher clock rates, ideally above 3200Mhz. | One should also go for a dual-channel or quad-channel setup to achieve ideal performance. | I decided to go for a dual-channel, G.Skill Trident RGB 16*2 3200Mhz RAM bundle. | . Note: Manufacturers make RAM sticks optimized specifically for either an Intel or AMD or both and it’s worth checking it out but as far as I know they don’t make as much difference in performance. . CPU Cooler . The hotly contested question that exists for CPU cooling is between air cooling and water cooling. | It’s well-founded that a good beefy Air Cooler can provide as good or even better cooling performance for a much cheaper cost at the expense of fan noise. | An AIO(All-in-one) cooler does provide the advantage of being much quieter as well as adds the aesthetics factor. | In my initial build, I used the stock cooler AMD Prism cooler but then succumbed to the urge of adding more RGB and aesthetics to my build and opted to buy the NZXT Kraken X63 cooler. | . . It’s worth noting that an AIO is the hardest component to fix due to the work needed in orienting it and getting the bracket right. | Do ensure to get the latest AM4 bracket if going for an AMD build as they are more secure, this was what was missing in the initial X62 AIO I ordered at first but later exchanged it for the later X63 model which was the same cost as the predecessor. | . Cabinet/ Case . Choosing a case/cabinet is a matter of preference at the end of the day. | Ensure that your case has decent airflow reviews which would be needed to keep your components at an ideal temperature and supports your expansion needs. | I wanted to get the Phanteks P400 Digital or Fractal Meshify C which has one of the best airflow’s at a decent cost but due to lack of availability with the NZXT 510i which is the great minimal looking case and provides a superb building experience. | The NZXT 510i lacks a front mesh and is thus less optimal in maintaining temperatures when compared to both my initial picks but is surely an eye-catching minimal case with great cable management, RGB capabilities, and newer-IO ports to make up for. | . . Cable Management . Cable management is hands down the most time-consuming and frustrating task in a build especially if you’re obsessed like me in getting things to look right. | Taking a piece of paper and plotting the wire path helps greatly in the process along with cable ties, velcro wraps, and tape. | . Before: . . After: . . . Complete Setup Parts List . CPU: AMD Ryzen 7 3700x | Motherboard: Gigabyte Aorus X570 Ultra | GPU: Gigabyte NVIDIA RTX 2070 Super Windforce OC | RAM: G.Skill Trident RGB 3200Mhz C16 * 2 | CPU Cooler: NZXT Kraken X63 | Case: NZXT 510i | PSU: Antec HCP 1000W | Monitor: BenQ GW2480 1080p 60Hz Monitor * 2 | Mouse: Logitech MX Master 2S | Keyboard: Ducky One 2 SF Mechanical Keyboard | . . Conclusion . All in all, building my own workstation from the ground up has been an amazing learning experience and is definitely something I would wholesomely suggest it to anyone interested in pursuing a career in data science. There’s definitely a learning curve to get a grip of things but once you get past that it becomes very clear as to why PC building is such a popular hobby for people all over the world. The path of curiosity and desire to learn all there is to learn about PC building to getting the parts and building your own system and seeing a successful post screen is definitely an invaluable experience to cherish! . By having a powerful workstation on hand has definitely made it very easy to prototype quickly without having to run through the hoops of setting the necessary requirements for your work, since that’s just a one-time process. . I’ll be following up this blog with another blog post detailing my setup process of getting all the necessary ML libraries installed, stress testing/ bench-marking my setup and setting up a remote server workstation. . . . Feel free to reach out to me regarding my work, to collaborate on ML projects or just discuss ML in general on my Twitter handle. . Stay safe and happy learning! :) . .",
            "url": "https://harish3110.github.io/through-tinted-lenses/workstation%20setup/2020/06/21/Introducing-Prometheus,-my-very-own-deep-learning-arsenal.html",
            "relUrl": "/workstation%20setup/2020/06/21/Introducing-Prometheus,-my-very-own-deep-learning-arsenal.html",
            "date": " • Jun 21, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Improving baseline model",
            "content": ". Introduction . In my previous blog post &quot;Building an image classifier using Fastai V2 &quot;, I introduced the Fastai V2 library by building a baseline flower classifier model on the flowers-102 dataset. . The baseline model created achieved an accuracy of about 96.2% on the validation set. The main aim of this post is to improve on my current baseline by employing some of the strategies taught in the Fastai course in order to reach the SOTA for the flowers-102 dataset which is an accuracy of 99.7%. . . Let&#39;s first import the Fastai V2 library: . from fastai2.vision.all import * %matplotlib inline . . Re-loading the baseline model . In the previous post, I saved the model created in order to re-load it again when needed. A saved model basically saves the weights and bias matrices for each of the layers of the architecture trained for the specific task. When the model is saved, behind the scenes Fastai uses saves the models state_dict and stores it in a pickle file. . To reload the model we could simply do: learn.load(&#39;flowers-baseline&#39;) . But hold on, it isn&#39;t quite that simple. . To load all those weight matrices properly we need to clearly define the architecture of the model as it was defined while training the model. So we follow the same steps as we did last time to create the DataBlock, then creating the dataloaders from it and finally specify the architecture we used by creating the Learner. . path = untar_data(URLs.FLOWERS) df = pd.read_csv(&#39;data/df.csv&#39;, index_col=0) . files = get_image_files(path/&#39;jpg&#39;) . def get_x(r): return path/r[&#39;name&#39;] def get_y(r): return r[&#39;class&#39;] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=RandomSplitter(seed=42), get_x= get_x, get_y= get_y, item_tfms = Resize(224)) dls = dblock.dataloaders(df) . learn = cnn_learner(dls, resnet34, metrics=accuracy) . learn.load(&#39;flowers-baseline&#39;) . &lt;fastai2.learner.Learner at 0x7f2ec9c62250&gt; . To view the weight and bias matrices of our model we can check its state_dict as follows: . learn.model.state_dict() . Note: When we do a model.save behind the scenes Fastai is calling upon PyTorch to save this state_dict. Something along the lines of this:torch.save(learn.model.state_dict(), &#39;checkpoint.pkl&#39;) . This is what is also happening when we use a pretrained model. We are basically taking the state_dict of a model trained for hours together and using it for our purposes! . Now that our pre-trained model is loaded, lets check where we stand by interpreting and validating our model on the validation set. . . Model interpretation . interp = ClassificationInterpretation.from_learner(learn) . One way to see how good your model is doing by using the confusion matrix which basically plots the matrix of correct predictions of the each class as compared to others. This means we can see the classes that were mistaken as the other by looking at a single class. You can do so in Fastai by running the following code: . interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . But in this case reading a confusion matrix would actually be pretty confusing since there are 102 different classes. So instead Fastai&#39;s most_confused will be used. This plots just the classes which were most frequently classified wrong which can be considered as a more refined and consise version of the confusion matrix. . So since all we want to know is which classes were moslty classified we can use Fastai&#39;s most_confused method as follows: . interp.most_confused(min_val=2) . [(&#39; sword lily&#39;, &#39; trumpet creeper&#39;, 3), (&#39; carnation&#39;, &#39; sweet william&#39;, 2), (&#39; petunia&#39;, &#39; tree mallow&#39;, 2), (&#39; sword lily&#39;, &#39; sweet pea&#39;, 2), (&#39; watercress&#39;, &#39; peruvian lily&#39;, 2)] . . Model prediction . Let&#39;s see how our model fares on new images taken from Google. I&#39;m using an image of a sunflower here: . flower = PILImage.create(&#39;images/sunflower.jpg&#39;) flower.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2ede450c10&gt; . learn.predict(flower)[0] . &#39; sunflower&#39; . preds,targs = learn.get_preds() . Let&#39;s use the accuracy method in Fastai to check the validation accuracy of the model: . accuracy(preds, targs) . TensorCategory(0.9627) . This is same as using the validate on the learner: . learn.validate() . (#2) [0.13905669748783112,0.9627367258071899] . So that&#39;s our baseline model of an accuracy of 96.2%. Let&#39;s try to improve upon it! . . Presizing . In the previous post, I explain how the images passed to the GPU needs to have the same dimensions so that they can be collated into tensors and effectively transferred onto to GPU. Thus at the very least doing a Resize operation in the item_tfms when creating the DataBlock is mandatory. . But if you think about it, simply resizing images is not a great idea. This might lead to producing improper cropped images, or images with empty areas or only parts of an image. . Also, doing all these transforms on a GPU is an expensive process, so finding an effective way to get this done by composing the augmentations to reduce the number of transforms being done would also be quite helpful . So to get over these issues the Fastai library employs a method called presizingas follows: . We choose an appropriate large enough size based on our dataset to resize to such that there is enough room for further augmentations to be doe without any loss of data. . | All the necessary augmentations are composed and performed together on the GPU and finally resized to our desired size. . Note: In practice, the crop area chosen for the first step is at random for the training set and a center crop is done for the test images. . | Let&#39;s check the size of a couple images in dataset to get an idea of what size to actually resize to in the 1st step: . print(PILImage.create(files[0]).shape) print(PILImage.create(files[10]).shape) print(PILImage.create(files[100]).shape) . (500, 666) (500, 620) (500, 667) . We see that the images are rectangle images with a fixed width of 500 and varying height. We can either work with rectangle images for our model but based on Jeremy&#39;s experimentation, in practice working with square images makes computation faster and produces almost similar results. . So lets choose a size of 460*460 that&#39;s smaller than the original image and gives us enough wiggle room to work with for the augmentations. Let&#39;s see how that resize looks like: . Note: By default, Fastai does a random crop when calling the resize function. . img = PILImage.create(files[100]) show_image(img, title=&#39;original image&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2ec9881110&gt; . _,axs = plt.subplots(1,3,figsize=(12,4)) for ax in axs: rsz = Resize(460, method=ResizeMethod.Crop) show_image(rsz(img), title=&#39;resized image&#39;, ctx=ax); . We then use aug_transforms which is a Fastai utility function to perform a list of transforms efficiently on images as follows: . batch_tfms=aug_transforms(size=224, min_scale=0.75) . Here by choosing a size of &#39;224&#39; and min_scale != 1, We are telling Fastai to perform a random resized crop on the images to the final size of 224*224. . Note: A RandomResizedCrop by default does a random crop of images in the training set and does a center crop of images in the validation set. . # Presizing item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75) def get_x(r): return path/r[&#39;name&#39;] def get_y(r): return r[&#39;class&#39;] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=RandomSplitter(seed=42), get_x= get_x, get_y= get_y, item_tfms = item_tfms, batch_tfms = batch_tfms) dls = dblock.dataloaders(df) . # Sanity check: Viewing a batch of images dls.show_batch(nrows=1, ncols=3) . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 2.834819 | 0.704838 | 0.820403 | 00:32 | . epoch train_loss valid_loss accuracy time . 0 | 0.703043 | 0.239197 | 0.939524 | 00:29 | . 1 | 0.335513 | 0.150513 | 0.960293 | 00:29 | . 2 | 0.159298 | 0.122102 | 0.971900 | 00:29 | . learn.save(&#39;flowers-presizing&#39;) . We can see that Presizing improved the accuracy of the model by almost 1% (from 96% to 97%), we can also see that the validation loss is also reducing and so is the training loss. . In practice, Fastai&#39;s recommendation is to always use presizing as it siginficantly improves model performance as the images obtained by doing all the destructive augmentation steps in a composed and efficient manner leads to better quality images. Additionally, by composing these transforms together there is an additional advantage of the model training faster overall. . . Learning rate finder . We know that choosing a good learning rate for the training process is vital. Till now we were using the default learning rate used in Fastai but let&#39;s dig deeper and choose a more suitable one for training the model more efficiently. . By default in Fastai the learning rate when using cnn_learner is set to 10e-3 i.e. 0.001 as shown below in its documentation. . item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75) def get_x(r): return path/r[&#39;name&#39;] def get_y(r): return r[&#39;class&#39;] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=RandomSplitter(seed=42), get_x= get_x, get_y= get_y, item_tfms = item_tfms, batch_tfms = batch_tfms) dls = dblock.dataloaders(df) . Fastai employs the popular learning rate finder trick as created by Leslie Smith to find an optimum learning rate to train a neural network model. In short the learning rate finder is a plot of the loss of our model over an epoch by gradually increasing our learning rate from a low value at each mini-batch to a very high value until the loss blows off. Then a smooth version of this plot is used to determine the region where the &#39;best learning&#39; takes place and accordingly an ideal learning rate is decided upon. . This is done in Fastai as follows: . learn = cnn_learner(dls, resnet34, metrics=accuracy) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 1.20e-02, steepest point: 7.59e-03 . From the above plot we can see that the plot has areas where loss is plateaud at first(i.e. no learning is taking place), then there is a gradual descent where the bulk of learning takes place and finally the loss shoots up and model goes out of wack! . We would like to choose a learning rate somewhere in the region where there&#39;s the gradual descent, not too low where the training process is waning but somewhere in the middle where the slope is high enough to induce a good learning for the model. . In this learning rate plot it appears that a learning rate around 3e-3 has a good learning slope and would be appropriate, so let&#39;s choose that. . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fine_tune(3, base_lr=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 2.415578 | 0.503028 | 0.872327 | 00:26 | . epoch train_loss valid_loss accuracy time . 0 | 0.548592 | 0.269859 | 0.924252 | 00:29 | . 1 | 0.279920 | 0.125399 | 0.968845 | 00:29 | . 2 | 0.116967 | 0.111194 | 0.969456 | 00:29 | . Note: : Everytime you change something in the model and need to retrain again, you will need to reset the learner. . To know more about finding a good learning rate, the thought process and code behind the learning rate finder in Fastai, please refer to Sylvain Gugger&#39;s wonderful post on &quot;How Do You Find A Good Learning Rate&quot; . . 1 Cycle training . Another novel approach of training brought about easily using the Fastai library is Leslie Smith&#39;s 1cycle training. . In summary this method is executed in 2 phases: . warm-up: This is where during the training the learning rate is gradually increased from a the the 10th of the learning rate decided by using the lr_finder to the our chosen one, i.e 3e-3. . | annealing: This is where the learning rate is reduced to gradually again to a much lower point that the original in order to find the best minima to finally land on. . | This method allows us train the major chunk of the model at higher learning rates using the learning rate restriction we had already calculated using the lr_finder. This, in practice has proven to show that models train much faster and also train better as they generalize better by missing dubious local minimas and thus finding smoother regions in the curve. . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.991562 | 0.425063 | 0.882712 | 00:26 | . 1 | 0.632092 | 0.185716 | 0.949297 | 00:26 | . 2 | 0.286753 | 0.159011 | 0.954795 | 00:26 | . learn.unfreeze() . We unfreeze the previous pretrained layers and train them specifically for our task of classifying flowers to fine-tune them for our flower classifier. . learn.lr_find() . SuggestedLRs(lr_min=3.0199516913853586e-06, lr_steep=6.309573450380412e-07) . We can see that this plot is different than before as we are finding the best minima to land on an already trained model. Let&#39;s choose an LR somewhere in the middle before the loss explodes again! . Something like 1e-5 as the lr_max would do good here in order to avoid the spike in loss that follows. Let&#39;s train the model further. . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss accuracy time . 0 | 0.222797 | 0.152495 | 0.961515 | 00:29 | . 1 | 0.204797 | 0.134804 | 0.967624 | 00:29 | . 2 | 0.166391 | 0.127007 | 0.968235 | 00:29 | . 3 | 0.148998 | 0.121040 | 0.968845 | 00:29 | . 4 | 0.141580 | 0.117179 | 0.967624 | 00:29 | . 5 | 0.140350 | 0.116889 | 0.968845 | 00:29 | . In addition to these steps, the 1 cycle policy also implements the cyclical momentum trick as mentioned by Leslie Smith in his paper. . Side Note: Momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also continues in the direction of previous steps. This allows the model training to move smoothly considering our previous steps taken and not being solely determined by the gradient of a single batch. Leslie Smith suggests that momentum and learning rate should be inversely proportional in the training process. I.e. when we are at high learning rate, we use less momentum, and we use more again in the annealing phase. By doing so, the learning rate is given higher priority in the tail end of warm up to go in new directions to find the flatter area and then slowly move to the best minima while giving momentum greater priority. . In practice Fastai shifts between a max and minimum momentum of 0.85 to 0.95 as mentioned by Leslie in the two phases of training. . We can view the learning rates and momentum plots during training as follows: . learn.recorder.plot_sched() . We can clearly see that the plots between learning rate and momentum are inversely proportional as explained above. . Let&#39;s view the plot_loss graph to see how well the training is going on: . learn.recorder.plot_loss() . We can see that the training loss and validation loss is stil decreasing and we get an accuracy of about 97%. . To learn more about the 1cycle training approach refer to Sylvain&#39;s post on it or chapter 13 of the Fastbook. . . Discriminative learning rates . In my previous post I explain how every CNN has a &quot;feature extraction&quot; section and &quot;classifier&quot; section and when we use transfer learning we just save the feature extracted part as that&#39;s the part we really care about and would like to use for our specific application. . Thus the main idea behind using discriminative learning rates is that, when using pretrained models the features learnt by the model in the early layers are very very useful and we wouldn&#39;t want to change it too much. We would like to use the best learning rate for the deeper layers where there can be a divergence in the features learnt based on our specific classification task. Therefore it doesn&#39;t make sense to use a single learning rate all over the architecture. Thus we use a smaller learning rate for the earliest layers and gradually increase the learning rate as we step through the layers. . Fastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value past will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let&#39;s use this approach to replicate the previous training, but this time we&#39;ll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let&#39;s train for a while and see what happens. . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 1.947487 | 0.377575 | 0.901649 | 00:30 | . 1 | 0.642104 | 0.197779 | 0.943189 | 00:30 | . 2 | 0.304672 | 0.171208 | 0.954184 | 00:30 | . epoch train_loss valid_loss accuracy time . 0 | 0.214030 | 0.164023 | 0.954184 | 00:38 | . 1 | 0.210405 | 0.149178 | 0.954795 | 00:38 | . 2 | 0.191148 | 0.134190 | 0.961515 | 00:38 | . 3 | 0.166917 | 0.125935 | 0.965791 | 00:38 | . 4 | 0.127839 | 0.116414 | 0.965180 | 00:38 | . 5 | 0.117447 | 0.107030 | 0.970678 | 00:38 | . 6 | 0.097778 | 0.104400 | 0.970678 | 00:38 | . 7 | 0.090379 | 0.101584 | 0.971900 | 00:38 | . 8 | 0.086792 | 0.101637 | 0.973122 | 00:38 | . 9 | 0.076258 | 0.099114 | 0.973732 | 00:38 | . 10 | 0.083770 | 0.096863 | 0.974954 | 00:38 | . 11 | 0.080138 | 0.099167 | 0.974954 | 00:38 | . learn.recorder.plot_loss() . Inference as taken from Fastbook:&quot;As you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Have a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we&#39;ve given the computer to help us to optimise.&quot; . . Number of epochs of training . Let&#39;s try training the model a little more and see what happens: . learn.fit_one_cycle(3, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.076480 | 0.099132 | 0.972511 | 00:38 | . 1 | 0.071239 | 0.100987 | 0.971289 | 00:38 | . 2 | 0.066199 | 0.099734 | 0.971289 | 00:38 | . learn.recorder.plot_loss() . We can see that the number of epochs of training is quite important as shown in this case. . Training a lot will mean we overfit as shown by the large divergence between training loss and validation loss as shown above. Here training loss is reducing but accuracy on validation set as well as validation loss has increased drastically again. This indicates that the model isn&#39;t generalizing well to the 102 classes of flowers but instead trying to memorize the images of the specific flowers per class in our training set. . Note: The most important value we care about is our metric i.e. our accuracy in this case! The validation loss is a created metric in order for the learning process to take place, so even if the validation loss reduced but our main metric i.e. accuracy increases then our training is still on the right track! . So in order to find the ideal number of epochs to train Fastai suggests to employ the fit_one_cycle method, unfreeze and train a set number of epochs till you find that your model metric is still reducing and the training is going on well. The point where you see that metric is going wack and not training properly anymore is where you have gone too far and need to start training again till an epoch or two before! . . Deeper architectures . Till now we have used a resnet34 architecture which indicates that it has 34 layers. . &quot;In general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorise the specific details of your individual images.&quot; - Fastbook Chapter 5 . Let&#39;s train a deeper pretrained model in the form of resnet50 which has 50 layers as compared to the 34 layers used till now. . from fastai2.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=accuracy).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss accuracy time . 0 | 3.017138 | 0.810731 | 0.806964 | 00:36 | . 1 | 1.019446 | 0.325558 | 0.908369 | 00:37 | . 2 | 0.508136 | 0.239720 | 0.937080 | 00:37 | . epoch train_loss valid_loss accuracy time . 0 | 0.231981 | 0.154821 | 0.951130 | 00:47 | . 1 | 0.177857 | 0.141960 | 0.960293 | 00:47 | . 2 | 0.151233 | 0.114160 | 0.967013 | 00:47 | . 3 | 0.077052 | 0.063637 | 0.979230 | 00:48 | . 4 | 0.031044 | 0.055957 | 0.983506 | 00:47 | . 5 | 0.022793 | 0.051753 | 0.984117 | 00:47 | . learn.recorder.plot_loss() . learn.save(&#39;flowers-resnet50&#39;) . learn.load(&#39;flowers-resnet50&#39;) learn.validate() . (#2) [0.051753416657447815,0.9841172695159912] . We can see that a deep resnet 50 architecture surely helps in bringing up the accuracy of the model to just over 98%. . Note: This isn&#39;t always the case and using deeper architectures could lead to worser models. &quot;Bigger models aren&#39;t necessarily better models for your particular case! Make sure you try small models before you start scaling up.&quot; - Fastbook Chapter 5 . Let&#39;s try to fine-tune this deeper resnet50 model by using the strategies we used earlier on: . learn.lr_find() . SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=7.585775847473997e-07) . learn.unfreeze() learn.fit_one_cycle(3, lr_max=slice(5e-6,3e-6)) . epoch train_loss valid_loss accuracy time . 0 | 0.011699 | 0.047581 | 0.986561 | 00:40 | . 1 | 0.012854 | 0.044599 | 0.989615 | 00:41 | . 2 | 0.013657 | 0.044380 | 0.990226 | 00:41 | . After a bit more of training the model reaches the 99% mark all while the training loss and validation loss is still reducing. . Let&#39;s save this as the final model for further use and set it as our current benchmark model on the flowers-102 dataset. . learn.save(&#39;flowers-resnet50-best&#39;) . learn.validate() . (#2) [0.04437994956970215,0.9902260303497314] . . Conclusion . In this post, we have successfully seen how to take a baseline model and improve upon it considerably. . We started with a baseline model at 96% accuracy and after employing some Fastai suggested methods we were able to build a model with an accuracy of 99%. All this is acheived by using the inbuilt convenience functions used in Fastai by using transfer learning. . There are definitely a load other tricks that can be done to this model to make it more robust and generalize better like employing tricks like progressive resizing, test time augmentation, mixup, adding regularization like dropout and using weight decay. . I&#39;m currently working on getting a better grasp of all these techniques and will be posting more about them in the upcoming blogs! . . References . Fastai v2 Documentation . | Fastbook i.e Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD by Jeremy Howard and Sylvain Gugger . | How do you find a good learning rate post by Sylvain Gugger | 1 cycle policy post by Sylvain Gugger | Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates paper by Leslie Smith. . | Cyclical Learning Rates for Training Neural Networks paper by Leslie Smith. . | . Happy learning! Stay home and stay safe!:) . .",
            "url": "https://harish3110.github.io/through-tinted-lenses/image%20classification/2020/04/10/Improving-baseline-model.html",
            "relUrl": "/image%20classification/2020/04/10/Improving-baseline-model.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "A look at Fastai's 'L' data structure",
            "content": ". The actual documentation for L can be found here. . L is defined in fastcore repo and generated from the &#39;01_foundation.ipynb&#39; notebook. . . Importing the libraries needed: . from fastcore.imports import * from fastai2.vision.all import * . . What is the &#39;L&#39; data structure? . In my previous blog I gave a brief intro to L when building an image classifier using Fastai V2 which you can find here . L is a special Fastai datastructure made specifically to handle with ease the model building in the library. Since, it forms the basic foundation to using the library, it&#39;s worth digging deeper into its functionalities. . It&#39;s worth noting that when trying to dig deeper into an aspect of a Python library like L for instance, it&#39;s critical to know the basic concepts of Object Oriented Programming. This is because everything in Python is written as a Class. To get a good understanding of OOPs in Python, you can check out this great free resource here . . Creating an L . Creating an instance &#39;a&#39; of class L from a list or any other normal iterable: . a = L([1, 2, 3]) a . (#3) [1,2,3] . We can use Python&#39;s isinstance method to check if a is an instance of L: . isinstance(a, L) . True . For Creating an &#39;L&#39; from an array or tensor we need to pass use_list=True since it doesn&#39;t iterate over them on construction. . By default we get something like this: . L(array([0.,1.1])) . (#1) [array([0. , 1.1])] . And when using use_list we get: . L(array([0.,1.1]), use_list=True) . (#2) [0.0,1.1] . # To see a realtime example when working with datasets in Fastai path = untar_data(URLs.PETS) Path.BASE_PATH = path files = get_image_files(path) . Here files is an L which can be also be checked by looking at its type as follows: . type(files) . fastcore.foundation.L . Let&#39;s take a look a better look at the class L using the help function: . help(L) . Help on class L in module fastcore.foundation: class L(CollBase) | L(self, items=None, *rest, use_list=False, match=None) | | Behaves like a list of `items` but can also index with list of indices or masks | | Method resolution order: | L | CollBase | builtins.object | | Methods defined here: | | __add__(a, b) | | __addi__(a, b) | | __contains__(self, b) | | __eq__(self, b) | Return self==value. | | __getitem__(self, idx) | Retrieve `idx` (can be list of indices, or mask, or int) items | | __init__(self, items=None, *rest, use_list=False, match=None) | Initialize self. See help(type(self)) for accurate signature. | | __invert__(self) | | __iter__(self) | | __mul__(a, b) | | __radd__(a, b) | | __repr__(self) | Return repr(self). | | __setitem__(self, idx, o) | Set `idx` (can be list of indices, or mask, or int) items to `o` (which is broadcast if not iterable) | | append(self, o) | Passthru to `list` method | | argwhere(self, f, negate=False, **kwargs) | Like `filter`, but return indices for matching items | | attrgot(self, k, default=None) | Create new `L` with attr `k` of all `items` | | cat(self: fastcore.foundation.L, dim=0) | Same as `torch.cat` | | clear(self) | Passthru to `list` method | | concat(self) | Concatenate all elements of list | | copy(self) | Same as `list.copy`, but returns an `L` | | count(self, o) | Passthru to `list` method | | cycle(self) | Same as `itertools.cycle` | | enumerate(self) | Same as `enumerate` | | filter(self, f, negate=False, **kwargs) | Create new `L` filtered by predicate `f`, passing `args` and `kwargs` to `f` | | index(self, value, start=0, stop=9223372036854775807) | Passthru to `list` method | | itemgot(self, *idxs) | Create new `L` with item `idx` of all `items` | | map(self, f, *args, **kwargs) | Create new `L` with `f` applied to all `items`, passing `args` and `kwargs` to `f` | | map_dict(self, f=&lt;function noop at 0xb1f55f488&gt;, *args, **kwargs) | Like `map`, but creates a dict from `items` to function results | | map_zip(self, f, *args, cycled=False, **kwargs) | Combine `zip` and `starmap` | | map_zipwith(self, f, *rest, cycled=False, **kwargs) | Combine `zipwith` and `starmap` | | pop(self, o=-1) | Passthru to `list` method | | product(self) | Product of the items | | reduce(self, f, initial=None) | Wrapper for `functools.reduce` | | remove(self, o) | Passthru to `list` method | | reverse(self) | Passthru to `list` method | | shuffle(self) | Same as `random.shuffle`, but not inplace | | sort(self, key=None, reverse=False) | Passthru to `list` method | | sorted(self, key=None, reverse=False) | New `L` sorted by `key`. If key is str then use `attrgetter`. If key is int then use `itemgetter` | | stack(self: fastcore.foundation.L, dim=0) | Same as `torch.stack` | | starmap(self, f, *args, **kwargs) | Like `map`, but use `itertools.starmap` | | sum(self) | Sum of the items | | tensored(self: fastcore.foundation.L) | `mapped(tensor)` | | unique(self) | Unique items, in stable order | | val2idx(self) | Dict from value to index | | zip(self, cycled=False) | Create new `L` with `zip(*items)` | | zipwith(self, *rest, cycled=False) | Create new `L` with `self` zip with each of `*rest` | | - | Class methods defined here: | | range(a, b=None, step=None) from fastcore.foundation.NewChkMeta | Same as `range`, but returns an `L`. Can pass a collection for `a`, to use `len(a)` | | split(s, sep=None, maxsplit=-1) from fastcore.foundation.NewChkMeta | Same as `str.split`, but returns an `L` | | - | Data and other attributes defined here: | | __hash__ = None | | __signature__ = &lt;Signature (self, items=None, *rest, use_list=False, m... | | - | Methods inherited from CollBase: | | __delitem__(self, i) | | __len__(self) | | - | Data descriptors inherited from CollBase: | | __dict__ | dictionary for instance variables (if defined) | | __weakref__ | list of weak references to the object (if defined) . We can see that L inherits from class CollBase. . . CollBase . We can see that L&#39;s Method resolution order: . L | CollBase | builtins.object | . Fastai source code defines CollBase as a &quot;Base class for composing a list of items&quot; . We can check is issubclass method in Python to check if a particular class is a subclass of another: . issubclass(L, CollBase) . True . When we look at the source code for L again using ?? in Jupyter we can see in the initialization a line: . super().__init__(items) . This indicates that if any items is passed to L, for instance a list, the initialization is handled by CollBase. . Now if you look at the source code of CollBase we can see the same as well as some other methods to deal with the items. . From the source code we can gather that CollBase has a bunch of methods to deal with the items added to it such as it&#39;s creation, the manipulation of the items. . . The methods available in L . Let&#39;s now look at all the methods available in L using the dir method in Python: . dir(L) . [&#39;__add__&#39;, &#39;__addi__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dict__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__invert__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__module__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__radd__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__signature__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_default&#39;, &#39;_get&#39;, &#39;_new&#39;, &#39;_xtra&#39;, &#39;append&#39;, &#39;argwhere&#39;, &#39;attrgot&#39;, &#39;cat&#39;, &#39;clear&#39;, &#39;concat&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;cycle&#39;, &#39;enumerate&#39;, &#39;filter&#39;, &#39;index&#39;, &#39;itemgot&#39;, &#39;map&#39;, &#39;map_dict&#39;, &#39;map_zip&#39;, &#39;map_zipwith&#39;, &#39;pop&#39;, &#39;product&#39;, &#39;range&#39;, &#39;reduce&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;shuffle&#39;, &#39;sort&#39;, &#39;sorted&#39;, &#39;split&#39;, &#39;stack&#39;, &#39;starmap&#39;, &#39;sum&#39;, &#39;tensored&#39;, &#39;unique&#39;, &#39;val2idx&#39;, &#39;zip&#39;, &#39;zipwith&#39;] . . We can see that the methods associated with can be clearly distinguished into two parts: . Methods starting and ending with __: These are refered to a dunder or magic methods of a class in Python . | Methods starting with _: These are methods defined specifically in the Fastai library. . | Normal methods: These are the normal methods defined to a class in Python. . | . 1. Special/ dunder/ magic methods . These special methods are used to emulate built-in methods in python that contributes with to ease of usability. . Since everything in Python is basically a class we can use these in-built methods for our own class by implementing something known as &#39;operator overloading&#39; to modify its behaviour as we intend to. . . __getitem__ . We can check how each method works by using the &#39;??&#39; in jupyter notebooks after the method name . Or we can use the help method: . Help on method __getitem__ in module fastcore.foundation: __getitem__(idx) method of fastcore.foundation.L instance Retrieve `idx` (can be list of indices, or mask, or int) items . We can access the index of an L as follows: . a.__getitem__(0) . 1 . Or in the more appealing way by indexing as follows: . a[0] . 1 . In practical terms, in case we want to see one image file path when building a model we can do it as follows: . files[0] . Path(&#39;images/Egyptian_Mau_167.jpg&#39;) . Note:Internally in Python, when we index as iterable like L as hsown aboev, it internally calls the __getitem__ method. . . __setitem__ . Help on method __setitem__ in module fastcore.foundation: __setitem__(idx, o) method of fastcore.foundation.L instance Set `idx` (can be list of indices, or mask, or int) items to `o` (which is broadcast if not iterable) . __setitem__ also takes an index and the value we want to add and we can do it as follows: . a[1] = 0 a . (#3) [1,0,3] . Internally, the above method is doing the same as below: . a.__setitem__(1, 4) a . (#3) [1,4,3] . . __contains__ . Help on method __contains__ in module fastcore.foundation: __contains__(b) method of fastcore.foundation.L instance . a = L([1, 2, 3]); a . (#3) [1,2,3] . We can use __contains__ to check if an element is in L as follows: . 1 in a . True . Internally in Python, this is what is happening: . a.__contains__(1) . True . . __delitem__ . Help on method __delitem__ in module fastcore.foundation: __delitem__(i) method of fastcore.foundation.L instance . a = L([1, 2, 3]) . We can delete items in L as follows: . a.__delitem__(0) a . (#2) [2,3] . Or we can do it as follows: . del(a[0]) a . (#1) [3] . files[0] . Path(&#39;images/Egyptian_Mau_167.jpg&#39;) . . __mul__ . a = L([1, 2, 3]) . a*3 . (#9) [1,2,3,1,2,3,1,2,3] . The above code is same as doing: a.__mul__(3) . Internally in Python this is what is actually being called. . . 2. Fastai specific methods . The available special methods is Fastai&#39;s `L` class are: _default, _get, _new, _xtra . Out of these methods the most interesting method is _new so lets look at that: . _new . Help on function _new in module fastcore.foundation: _new(self, items, *args, **kwargs) . a = L([1, 2, 3]); a . (#3) [1,2,3] . help(a._new) . Help on method _new in module fastcore.foundation: _new(items, *args, **kwargs) method of fastcore.foundation.L instance . The _new creates a new instance of L which requires some items to be passed to it. . a . (#3) [1,2,3] . Using _new to create a soft copy of a . a = L([1, 2, 3]) . b = a._new(a.items) b . (#3) [1,2,3] . id(a), id(b) . (112557760920, 112557761760) . a.append(4) a . (#4) [1,2,3,4] . b . (#4) [1,2,3,4] . Note: The __mul__ function uses _new, it returns:a._new(a.items*b) . . 3. Normal methods . . append . a = L([1, 2, 3]) a.append(4) a . (#4) [1,2,3,4] . . arttrgot . This method can be used to get specific attributes from all items in L . Help on function attrgot in module fastcore.foundation: attrgot(self, k, default=None) Create new `L` with attr `k` of all `items` . attrgot is a simple function that just maps getattr and the attribute you pass to an L. . attrgot is a pretty useful function and here&#39;s a practical example of where you can use it to get the name or stem attribute from paths. . ten_file_names = files[:10].attrgot(&#39;name&#39;) ten_file_names . (#10) [&#39;Egyptian_Mau_167&#39;,&#39;pug_52&#39;,&#39;basset_hound_112&#39;,&#39;Siamese_193&#39;,&#39;shiba_inu_122&#39;,&#39;Siamese_53&#39;,&#39;Birman_167&#39;,&#39;leonberger_6&#39;,&#39;Siamese_47&#39;,&#39;shiba_inu_136&#39;] . The stem attribute calculates the name without the file extensions as well . ten_file_stems = files[:10].attrgot(&#39;stem&#39;) ten_file_stems . (#10) [&#39;Egyptian_Mau_167&#39;,&#39;pug_52&#39;,&#39;basset_hound_112&#39;,&#39;Siamese_193&#39;,&#39;shiba_inu_122&#39;,&#39;Siamese_53&#39;,&#39;Birman_167&#39;,&#39;leonberger_6&#39;,&#39;Siamese_47&#39;,&#39;shiba_inu_136&#39;] . . enumerate . Help on function enumerate in module fastcore.foundation: enumerate(self) Same as `enumerate` . ten_file_names.enumerate() . (#10) [(0, &#39;Egyptian_Mau_167&#39;),(1, &#39;pug_52&#39;),(2, &#39;basset_hound_112&#39;),(3, &#39;Siamese_193&#39;),(4, &#39;shiba_inu_122&#39;),(5, &#39;Siamese_53&#39;),(6, &#39;Birman_167&#39;),(7, &#39;leonberger_6&#39;),(8, &#39;Siamese_47&#39;),(9, &#39;shiba_inu_136&#39;)] . enumerate returns an L where each element is tuple with the index(0th indexing) and the item itself. . We can use it something like this in a loop to build something from it if in case we need the index as well as the item for some manipulations . for i, file in enumerate(ten_file_names): print(i, file) . 0 Egyptian_Mau_167.jpg 1 pug_52.jpg 2 basset_hound_112.jpg 3 Siamese_193.jpg 4 shiba_inu_122.jpg 5 Siamese_53.jpg 6 Birman_167.jpg 7 leonberger_6.jpg 8 Siamese_47.jpg 9 shiba_inu_136.jpg . . filter . Help on function filter in module fastcore.foundation: filter(self, f, negate=False, **kwargs) Create new `L` filtered by predicate `f`, passing `args` and `kwargs` to `f` . filter is used to return items in an L that pass a function. . For instance we can use it to get all the items in files which start with upper case. Based on the PETS dataset, all such names that start with uppercase indicates that its a cat. . ten_file_names.filter(lambda x: x[0].isupper()) . (#5) [&#39;Egyptian_Mau_167.jpg&#39;,&#39;Siamese_193.jpg&#39;,&#39;Siamese_53.jpg&#39;,&#39;Birman_167.jpg&#39;,&#39;Siamese_47.jpg&#39;] . with negate=True we can get the vice-versa, i.e. all the dog image files: . ten_file_names.filter(lambda x: x[0].isupper(), negate=True) . (#5) [&#39;pug_52.jpg&#39;,&#39;basset_hound_112.jpg&#39;,&#39;shiba_inu_122.jpg&#39;,&#39;leonberger_6.jpg&#39;,&#39;shiba_inu_136.jpg&#39;] . . map . Help on function map in module fastcore.foundation: map(self, f, *args, **kwargs) Create new `L` with `f` applied to all `items`, passing `args` and `kwargs` to `f` . map is used to map a function over all elements of L . L.range(4).map(lambda x: x**2) . (#4) [0,1,4,9] . . map_dict . Help on function map_dict in module fastcore.foundation: map_dict(self, f=&lt;function noop at 0xb220cc6a8&gt;, *args, **kwargs) Like `map`, but creates a dict from `items` to function results . map_dict applies a function over &#39;L&#39; to return a dict where the dictionary&#39;s items are the original elements and its values are the modified values after applying the function. . L.range(1, 4).map_dict(lambda x: x**2) . {1: 1, 2: 4, 3: 9} . . zip . Help on function zip in module fastcore.foundation: zip(self, cycled=False) Create new `L` with `zip(*items)` . a = L([1, 2, 3], &#39;abc&#39;) a . (#2) [[1, 2, 3],&#39;abc&#39;] . a.zip() . (#3) [(1, &#39;a&#39;),(2, &#39;b&#39;),(3, &#39;c&#39;)] . . split . L.split(&#39;a/b/c&#39;, sep=&#39;/&#39;) . (#3) [&#39;a&#39;,&#39;b&#39;,&#39;c&#39;] . . concat . Help on function concat in module fastcore.foundation: concat(self) Concatenate all elements of list . a = L([1, 2, 3, [&#39;abc&#39;]]) a . (#3) [1,2,[&#39;abc&#39;]] . a.append([&#39;def&#39;]) a . (#4) [1,2,[&#39;abc&#39;],[&#39;def&#39;]] . a.concat() . (#4) [1,2,&#39;abc&#39;,&#39;def&#39;] . b = L([[[1, 2], 3], [4, 5]]) b.concat().concat() . (#5) [1,2,3,4,5] . . Conclusion . The L data structure created in Fastai V2 is extremely useful and has some really power functionality. I am still digging into the depths of Fastai V2 and as I go along with it i&#39;ll try to update this in terms of the practical usability of the methods in its class for building state-of-the-art models using the library. . . Happy learning! Stay home and stay safe :) . .",
            "url": "https://harish3110.github.io/through-tinted-lenses/data%20structures/2020/04/03/A-look-at-the-L-Data-Structure-in-Fastai.html",
            "relUrl": "/data%20structures/2020/04/03/A-look-at-the-L-Data-Structure-in-Fastai.html",
            "date": " • Apr 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Building an image classifier using Fastai V2",
            "content": ". I would like to start these series of posts with an introduction to the fastai v2 library in the application of vision, which is arguably the most common application in the field, and definitely, the most worked on. . . Update: The follow-up post to this one can be found here where I employ Fastai techniques with an aim to build a SOTA model. . . Importing the library and necessary modules . from data.utils import * from fastai2.vision.all import * %matplotlib inline . Note: The first line imports some helper functions from utils.py as used in the fastbook repo which provides great visualization options. . . About the dataset . Let&#39;s start with the Flowers dataset, which is a common dataset for image classification tasks. The dataset is a collection of images of 102 different types of flowers, which is nicely curated. The images are of fairly reasonable size shot in different angles and lighting conditions. . Here&#39;s a list of the 102 different categories of flowers in this dataset for your reference. . . Getting and exploring the dataset . Now that we have our arsenal set up and have an understanding of the data we are working. Let&#39;s download it and explore it. . About 90% of the work done by data scientist revolves around clearly gathering data. For simplicity, let&#39;s begin with commonly available datasets. . Fastai library makes it extremely easy to get common well-know datasets and is stored in Amazon S3 buckets for fast retrieval and use. They&#39;re all stored in the URLs global constant. . flowers_link = URLs.FLOWERS flowers_link . &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-102-flowers.tgz&#39; . path = untar_data(flowers_link) path.ls() . (#4) [Path(&#39;/home/jupyter/.fastai/data/oxford-102-flowers/valid.txt&#39;),Path(&#39;/home/jupyter/.fastai/data/oxford-102-flowers/test.txt&#39;),Path(&#39;/home/jupyter/.fastai/data/oxford-102-flowers/jpg&#39;),Path(&#39;/home/jupyter/.fastai/data/oxford-102-flowers/train.txt&#39;)] . The above response may appear like a datastructure similar to a list in Python, but it&#39;s a built-in fastai data structure called &#39;L.&#39; You can think of it as a data structure, which is an amalgamation of lists and dicts in Python. . The above output begins with a tuple &#39;#4&#39;, which indicates that the path has 4 sub-directories in it and then displays those directories as an array. . To see the directories better, let&#39;s just see the base path of the sub directories instead of the entire path. . Path.BASE_PATH = path path.ls() . (#4) [Path(&#39;valid.txt&#39;),Path(&#39;test.txt&#39;),Path(&#39;jpg&#39;),Path(&#39;train.txt&#39;)] . # !pip install tree !tree -d {path} . /home/jupyter/.fastai/data/oxford-102-flowers └── jpg 1 directory . Now we can clearly see that the directory has one folder and three .txt files: . jpg: A folder containing all the images of the dataset | txt files: 3 text files indicating train, test, and validation. | . Let&#39;s look into the &#39;jpg&#39; folder: . Fastai provides an in-built function, get_image_files to get all image files in a folder as an &#39;L&#39;. . files = get_image_files(path/&#39;jpg&#39;) files . (#8189) [Path(&#39;jpg/image_03449.jpg&#39;),Path(&#39;jpg/image_05274.jpg&#39;),Path(&#39;jpg/image_03731.jpg&#39;),Path(&#39;jpg/image_07311.jpg&#39;),Path(&#39;jpg/image_05189.jpg&#39;),Path(&#39;jpg/image_06695.jpg&#39;),Path(&#39;jpg/image_06706.jpg&#39;),Path(&#39;jpg/image_04746.jpg&#39;),Path(&#39;jpg/image_04017.jpg&#39;),Path(&#39;jpg/image_06632.jpg&#39;)...] . We can see that there are 8189 images in the dataset available to us. Let&#39;s look at one of the images. . img = PILImage.create(files[0]) img.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcf6f598650&gt; . Now let&#39;s look at the .txt files using pandas. Pandas is the go-to method for dealing with a tabular structure in python and is an essential skill for all data scientists using python. . Pandas can read many formats of data such as CSV, Excel, as well as text files very quickly by creating a pandas dataframe for manipulating it accordingly. . train = pd.read_csv(path/&#39;train.txt&#39;, header=None, sep=&#39; &#39;) train.head() #head displays the first 5 rows of the dataframe . 0 1 . 0 jpg/image_03860.jpg | 16 | . 1 jpg/image_06092.jpg | 13 | . 2 jpg/image_02400.jpg | 42 | . 3 jpg/image_02852.jpg | 55 | . 4 jpg/image_07710.jpg | 96 | . We can see that the file contains the image file name and its corresponding labels. So let&#39;s label the columns of the pandas dataframe accordingly. . cols = [&#39;name&#39;, &#39;label&#39;] train.columns = cols train.head() . name label . 0 jpg/image_03860.jpg | 16 | . 1 jpg/image_06092.jpg | 13 | . 2 jpg/image_02400.jpg | 42 | . 3 jpg/image_02852.jpg | 55 | . 4 jpg/image_07710.jpg | 96 | . Now that we have an organized structuring for our training files. Let&#39;s do the same to create a validation and test dataframe. . # validation df valid = pd.read_csv(path/&#39;valid.txt&#39;, sep=&quot; &quot;, names= cols ) # test df test = pd.read_csv(path/&#39;test.txt&#39;, sep=&quot; &quot;, names= cols ) . valid.head() . name label . 0 jpg/image_04467.jpg | 89 | . 1 jpg/image_07129.jpg | 44 | . 2 jpg/image_05166.jpg | 4 | . 3 jpg/image_07002.jpg | 34 | . 4 jpg/image_02007.jpg | 79 | . test.head() . name label . 0 jpg/image_06977.jpg | 34 | . 1 jpg/image_00800.jpg | 80 | . 2 jpg/image_05038.jpg | 58 | . 3 jpg/image_06759.jpg | 0 | . 4 jpg/image_01133.jpg | 45 | . Let&#39;s see the count of images in each one of the train, validation and test datasets. . print(f&quot;The number of images in training set are:{len(train)}&quot;) print(f&quot;The number of images in validation set are:{len(valid)}&quot;) print(f&quot;The number of images in test set are:{len(valid)}&quot;) . The number of images in training set are:1020 The number of images in validation set are:1020 The number of images in test set are:1020 . We can see that we have a total of around 8000 labelled images to build our flower classifier of 102 different clases. Since the data we have ain&#39;t that much let&#39;s utilize all the data available to build our model. To do so let&#39;s first merge the 3 dataframes into one. . df = pd.concat([train, valid, test], axis=0) df.head() . name label . 0 jpg/image_03860.jpg | 16 | . 1 jpg/image_06092.jpg | 13 | . 2 jpg/image_02400.jpg | 42 | . 3 jpg/image_02852.jpg | 55 | . 4 jpg/image_07710.jpg | 96 | . The main dataframe consists of 8189 labeled images. . Let&#39;s now save this dataframe as a CSV file for ease of access later on. . df.to_csv(&#39;data/df.csv&#39;) . By looking at the above dataframe we can see that the images and numerically labelled by the creators of the datset, probably for easier mapping. This would make visualization pretty bad since we won&#39;t what flower we are seeing in the end. So let&#39;s get the mpping of the labels to the numerical assignemnt given here. . If you look at the main source of the dataset, the labels are provided in a .mat file and would be quite cumbersome to fetch. Luckily enough JosephKJ did the labelling and generously made it available for us here. I downloaded the .txt file and will read it using pandas. . labels = pd.read_csv(&#39;data/labels.txt&#39;, header=None, names = [&#39;labels&#39;]) labels[&#39;labels&#39;] = labels[&#39;labels&#39;].apply(lambda x: x.replace(&quot;&#39;&quot;, &quot;&quot;)) labels.head() . labels . 0 pink primrose | . 1 hard-leaved pocket orchid | . 2 canterbury bells | . 3 sweet pea | . 4 english marigold | . Now creating labels dictionary where key is the number and value is the respective name of the flower. . labels_dict = dict(zip(list(range(len(labels))), labels[&#39;labels&#39;])) . Let&#39;s use the all powerful pandas apply function again to map the numerical labels in df with the labels_dict . # Creating a new column &#39;class&#39; using the existing label of images df[&#39;class&#39;] = df[&#39;label&#39;].apply(lambda x: labels_dict[x]) df.head() . name label class . 0 jpg/image_03860.jpg | 16 | purple coneflower | . 1 jpg/image_06092.jpg | 13 | spear thistle | . 2 jpg/image_02400.jpg | 42 | sword lily | . 3 jpg/image_02852.jpg | 55 | bishop of llandaff | . 4 jpg/image_07710.jpg | 96 | mallow | . Now that we have arranged our data exactly as we want it, let&#39;s move ahead to model building using fastai library. . . The DataBlock API . We have our data structured well and exactly as we want. It&#39;s now time to feed it into the fastai library. This can be done using the DataBlocks API. . The DataBlocks API is the Fastai solution to simplifying the most time-consuming task in a data science pipeline, Data Preparation. It&#39;s easy to use, highly hackable and can be be used for a wide variety of data on applications such as vision, tabular, and text. . This DataBlock API is a much-needed addition to the fastai v2, which makes it super easy to load in data as needed for deep learning models. . def get_x(r): return path/r[&#39;name&#39;] def get_y(r): return r[&#39;class&#39;] dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=RandomSplitter(seed=42), get_x= get_x, get_y= get_y, item_tfms = Resize(224)) dls = dblock.dataloaders(df) . Let&#39;s get a basic understanding of what&#39;s happening in the above code. Creating data which can be fed to a model requires two steps: . 1. Create a DataBlock: . A Datablock can be considered as a series of sequential functions all collated into one function. For people who have used sci-kit-learn, this can be considered similar to a pipeline. . The DataBlock API requires some methods to get the input data in the desired format for model building and training: . blocks: This is used to define the input and output of the model. In the above code, the type of input i.e. the independent variable, which are images and hence ImageBlock. The output, i.e., dependant variable, are categories of flowers and hence CategoryBlock. . | splitter: Splitters are used to divide our data into training and validation. This is of utmost importance because we don&#39;t want our model to train and memorize all the training images. We want a subset of images to validate how good our model is doing. In this case, we use a method that randomly splits data in train and validation. . | getters: Getters are used to get the independent and dependant variables in the right order. The two getters used are get_x and get_y. Here we defined get_x by grabbing the name of the image file from df and adding the path to it, and we set get_y by getting the class column from df. . | transforms: Transforms are used to perform data augmentation techniques on our input data either on the entire data(item_tfms) on the CPU or on the data passed as batches(batch_tfms) when passing it through the architecture on the GPU. In this case, we are just resizing all images to 224, which is mandatory as deep learning models need all our training images to be of the same size. . | . 2. Call the dataloaders method on your data: . A dataloaders is a method called on the DataBlock where we pass in our dataframe df to perform all the steps mentioned in the DataBlock, which finally returns the data in the required format for modeling. . . Visualizing the data . Now that we have made our data ready and in the format to be ingested by the fastai library, let&#39;s visualize our data. . We can use show_batch method from our dataloader created to visualize a batch of images and their labels. . dls.show_batch() . . Building the model . Let&#39;s build a deep learning model on our dataset using a Convolutional Neural Netwok(CNN) model. . Basic understanding of CNN&#39;s . Here&#39;s the general architecture of every CNN model . . Every CNN architecture consists of 4 parts: . Input layer: The input i.e. the image dataset with &#39;n&#39; classes which are correctly labeled on which our image classification model is built on. . | Feature extraction: This is the crux of the CNN model. It learns various features of the classes in your data and how to distinguish between them during the training process. For instance, during the training process, if images of dogs are passed in, the initial layers learn simple features such as lines, edges, circles. Still, as we move to later layers of the model, the model learns complex features like ears, nose, and eyes of the dog. In machine language, all these features are represented numerically, and we refer to all these learned features as parameters of the model. . | Classification: This part of the model is used to pool in the different features learned and associate it with the corresponding class. Continuing the example of our dog, it could mean that we pool in the features learned about the dog, such as its nose, eyes, tail, etc. and associate it with the output label, which is a dog. . | Output: This is the part that associates an input image to class as labeled in our training set. When a new image of a dog is sent through a trained model, the output class of the model will get activated and indicate that the input image is that of a dog. . | . The entire structure of the model built above is referred to as a model architecture in deep learning. . Now, we can build flower classifier from scratch, but since we only have 8000 odd images for 102 different classes. We can safely say that we don&#39;t have enough data to build a beautiful model from scratch. . Instead, let&#39;s use the biggest weapon in the deep learning arsenal available at our disposal, Transfer Learning. . . Transfer Learning . Transfer Learning is a method that uses the work done by other researchers who spend days on end to build appropriate architectures which train on large datasets for specific tasks. For instance, the ImageNet dataset, which consists of 1.3 million images of various sizes around 500 pixels across in 1000 categories, takes a few days to train. . The main idea is as follows, the ImageNet dataset has 1000 different everyday categories, and the parameters and features it learns for each one of the categories can be applied to make the building blocks of the flowers dataset we are working on. The initial feature extraction layers that learn simple features line lines, edges, circles, etc. can be applied to flowers as well, and we can fine-tune the final layers to distinguish between the different classes of flowers. . The final output layer(which is trained on 1000 categories) also needs to be removed and replaced with the 102 different classes of flowers. . . Defining the learner . In fastai, to build a model, we use the cnn_learner class. We need to pass in the following details to the cnn_learner to train the model: . Dataloaders object: The dls dataloader we created according to how fastai needs the input data. | Model Architecture: This is the architecture we would like to use. Since we are making use of transfer learning, we&#39;ll use a pretrained model, in this case, the famous resnet34 architecture. | Metrics: This is how you would like to keep track of your training progress. In this case, we&#39;ll use accuracy, which indicates how well our model classifies all the classes in our data overall. | . learn = cnn_learner(dls, resnet34, metrics=accuracy) . Let&#39;s visualize the model architecture used i.e the resnet34 architecture . learn.summary() . Sequential (Input shape: [&#39;64 x 3 x 224 x 224&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 64 x 112 x 112 9,408 False ________________________________________________________________ BatchNorm2d 64 x 64 x 112 x 112 128 True ________________________________________________________________ ReLU 64 x 64 x 112 x 112 0 False ________________________________________________________________ MaxPool2d 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 73,728 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 8,192 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 294,912 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 32,768 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 1,179,648 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 131,072 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 1024 0 False ________________________________________________________________ BatchNorm1d 64 x 1024 2,048 True ________________________________________________________________ Dropout 64 x 1024 0 False ________________________________________________________________ Linear 64 x 512 524,288 True ________________________________________________________________ ReLU 64 x 512 0 False ________________________________________________________________ BatchNorm1d 64 x 512 1,024 True ________________________________________________________________ Dropout 64 x 512 0 False ________________________________________________________________ Linear 64 x 102 52,224 True ________________________________________________________________ Total params: 21,864,256 Total trainable params: 596,608 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd0087225f0&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . If you scroll down and read the summary, in the end, you can see that there are a total of around 21 million parameters, but only around 600k parameters are trainable. . This is because we have inherited a model trained extensively on the ImageNet dataset and are using the parameters learned by that model to train our model on 102 classes of flowers. . This means that the parameters learned early on in the model to distinguish various objects are kept as is, and the final layers are replaced to classify. . Fine-tuning pretrained model . Once we have created our learner based on a pretrained resnet34 model, let&#39;s train our learner on classifying the flowers. In fastai there&#39;s a learner method specifically to train a pretrained model called fine_tune quickly. . fine_tune by default trains the head, i.e. the additional part to the model we added for our classification of 102 flowers for one epoch and then unfreezes the all the weights and optimizes the entire model including the weights in the starting phase. . So in the first epoch, the model learns the 600k trainable parameters such that all 21 million parameters are trained roughly. In the consecutive 2 epochs, it optimizes the all these weights specifically for the task in hand, i.e. classifying 102 types of flowers. . learn.fine_tune(3) . epoch train_loss valid_loss accuracy time . 0 | 2.779661 | 0.685666 | 0.831399 | 00:24 | . epoch train_loss valid_loss accuracy time . 0 | 0.625886 | 0.277132 | 0.927306 | 00:32 | . 1 | 0.256375 | 0.147598 | 0.959071 | 00:32 | . 2 | 0.100605 | 0.139057 | 0.962737 | 00:32 | . We can see that by using the power of transfer learning, in 2 lines of code and 3 steps of training(epochs) which took about 30 seconds to train on small GPU we are able to build a flowers classifier which can classify between 102 types of flowers with greater than 96% accuracy! . This blows the original paper out of the water which came out in 2008 which used a non-DL approach to tackle this problem and received an accuracy of about 72.8% . . But this isn&#39;t really a fair comparison, so if we compare our basic model the current leaderboard for the flowers-102 dataset, we can see that the best accuracy on the entire dataset is about 99.7%. . So, this is a good starting point. This will act as our baseline model for future improvements and experimentation in order to come closer to the current benchmark. . So let&#39;s save this model so that we can build on from here form next time. . learn.save(&#39;flowers-baseline&#39;) . We&#39;ll slowly build upon this and make this model better while learning more about building state-of-the-art models in the upcoming posts. . . Update: The follow-up post to this one can be found here where I employ Fastai techniques with an aim to build a SOTA model. . . . Learning resources . Corey Schafer&#39;s Python Tutorials | Corey Schafer&#39;s Pandas Tutorials | Fastai v2 Documentation . | Fastbook i.e Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD by Jeremy Howard and Sylvain Gugger . | Fastai DataBlock API walkthrough blog by Zach Mueller . | Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates paper by Leslie Smith . | Cyclical Learning Rates for Training Neural Networks paper by Leslie Smith . | . Happy learning, stay at home and stay safe! :) . .",
            "url": "https://harish3110.github.io/through-tinted-lenses/computer%20vision/image%20classification/2020/03/29/Building-an-image-classifier-using-Fastai.html",
            "relUrl": "/computer%20vision/image%20classification/2020/03/29/Building-an-image-classifier-using-Fastai.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Relearning how to learn, deep.",
            "content": "“The illiterate of the 21st century will not be those who cannot read and write, but those who cannot learn, unlearn, and relearn. ” ― Alvin Toffler — . Fact: Deep learning is hard! . I have a background in Electronics Engineering and having studied in the sub-continent, the amount of exposure I have had with programming was virtually non-existent. Since graduating, I have slowly transitioned my way into the field of data science and machine learning. I had to start from scratch and build block after block all the necessary skills needed to call myself a capable data scientist, and successfully land myself a job in the field. . Whenever I meet anyone from a non-ML background and tell them I’m a data scientist, their response is almost always the same. They nod in appreciation and say something like, “That’s the future!” and that “I’m on the right path!”. They then begin asking questions about the areas like self-driving cars and other popular fields and finally land on to the million-dollar question. “Is it difficult to become a machine learning/deep learning engineer?”. To this, I always find myself lying by spouting out of phrases like “it’s easy” and “anyone can do it”! Whereas the fact of the matter is that despite there being some credibility to the statement, the learning curve isn’t that easy. . Being a self-learner, I learned how to code in Python, understood the fundamental concepts of data science and machine learning by following an array of popular MOOCs such as Coursera, Udacity, etc. as well as completed certifications from reputed colleges in India. It has been an incredible journey of learning, but it has definitely not been a cakewalk! There is always a huge learning curve, and even after you complete a course or certification, there’s still that void of not having built anything meaningful. Every one of the courses makes a point of conveying the theory and underlying maths well but almost always fail at delivering to students the necessary tools to go ahead and build something practical. There’s always the next step(s) that needs to be taken to actually implement the knowledge learned in the course. Being able to understand the theory and math from ground up was satisfying to begin with but as you dig deeper, without being able to rapidly prototype and experiment with the concepts takes a toll on the learning process! . . Enter Fastai . I first heard about Fastai and Jeremy Howard from my friends around version 1 of the course. I pushed it aside as one of the many courses suggested by people in the field due to their inherent allegiance to it having taken it. But over time, on Twitter, LinkedIn, and other sources, it had reached a point that I couldn’t not take a look at the course. So I finally succumbed to the pressure and started last year’s course. . . In less than two weeks, I binged watch through the entire part 1! The course was nothing like I had taken before. All the concepts explained intuitively and efficiently, and more importantly, everything taught was visualised through code and by building state-of-the-art models. I fell in love with the teaching methodology and with the community it had garnered. I couldn’t but feel envious of the new students taking up the course to learn deep learning as I was comparing it to myself starting out in ML a year ago and how a course like this would have been extremely helpful to my past self. . . Fact: Deep learning is hard . Fact: Deep learning is easy if done right! . “There are Two Core Abilities for Thriving in the New Economy : . The ability to quickly master hard things. | The ability to produce at an elite level, in terms of both quality and speed.” ― Cal Newport | The main idea of the course and the library is to democratise this powerful tool of ‘Deep Learning’, in a way that it can be easily harnessed by people across all domains such that one can apply the principles easily in their domain. . The course challenges the usual way of learning by following a top-down approach to understanding deep learning. In comparison to every other DL course under the sun, this course makes the field easy to approach, and most importantly, it helps implement the models very quickly. Students taking the class learn to apply all the theoretical concepts learned immediately with concrete examples rather than learn mathematical proofs. In a rapidly evolving field such as this, being able to learn and rapidly prototype simultaneously is invaluable! . Everything taught in this year’s course is again application-driven and closely follows the book written by the founders of Fastai - Jeremy, and Sylvain. The only prerequisites needed to start with the course are high school math and intermediate coding skills in Python, which, to be honest, can be picked up along the way(I will be sure to put up references for all in the end). This doesn’t mean that the course is geared only for beginners. On the contrary, even veterans in the field will have a lot to discover and learn in the course. The course gradually wades through the ingenious implementations, tricks, and insights gained through experimentation by the Fastai team, which has led them to achieve state-of-the-art benchmark models by beating top companies with considerably limited compute resources as compared to big guns in the field. . The lectures taught by Jeremy, the book as a manual to wade through the ‘frightening’ depths of deep learning, the fantastic community of like-minded and extremely helpful peers, is a complete package and the perfect recipe to learn. . “What I hope is that lots of people will realise that state-of-the-art results of deep learning are something they can achieve even if they’re not a Stanford University deep learning PhD.” — Jeremy Howard . . My blog post series . There’s definitely a self-centred motivation to write these blog posts. Following the advice by Rachel Thomas(co-founder of fast.ai) in her blog post, she encourages anyone on a learning path to put out blog posts in order to maximise your learning. . But aside from the learning advantages, I would like to do my best in helping guide people entering this field on how best to navigate the myriad resources in the field and hopefully impart some of the knowledge learned along my journey until now. . Being quarantined, I find this time to be the best opportunity to start my blog by delving into depths of this fantastic deep-learning library! . The course is currently private but will be made public and free for all like all courses by fast.ai. Until then, and hopefully, even then, I hope I can help wade students through the mazes of Deep Learning, Fast.ai style! So let’s get into it! . In my upcoming blog post, I will provide an introduction to the Fastai v2 library and build an image classifier on a well-known dataset. . “Education is the kindling of a flame, not the filling of a vessel.” ― Socrates . . Happy learning, stay at home and stay safe! :) .",
            "url": "https://harish3110.github.io/through-tinted-lenses/fastai/2020/03/28/Relearning-how-to-learn-deep.html",
            "relUrl": "/fastai/2020/03/28/Relearning-how-to-learn-deep.html",
            "date": " • Mar 28, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://harish3110.github.io/through-tinted-lenses/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://harish3110.github.io/through-tinted-lenses/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}